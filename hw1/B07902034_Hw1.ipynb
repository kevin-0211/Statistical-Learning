{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習 (Fall, 2020)\n",
    "### Homework 1 \n",
    "B07902034 資工三 王昱凱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一題 [myknn_regressor]\n",
    "#### Q1.1 Create your myknn_regressor based on the skeleton.\n",
    "\n",
    "以下為我實作的myknn_regressor架構，其中主要分為三個函式:\n",
    "* init( ) 是用來定義這個class會使用到的參數，比較特別的是我會先判斷n_neighbors是否大於10，接著才對mean_type做定義\n",
    "* fit( ) 是用來定義接下來prediction會使用到的data (x_train & y_train)\n",
    "* predict( ) 是主要實作knn的函式，首先先計算跟所有neighbors之間的距離，接著排序並找出前$k$小的值，再來找出對應的$y_a$，如果mean_type是remove_outliers的話，則計算$y_a$的第一四分位數和第三四分位數並且將$y_a$的範圍限制在$[Q1 - 1.5 IQR, Q3 + 1.5 IQR]$，最後計算$y_a$的平均即為結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "class myknn_regressor():\n",
    "    def __init__(self, n_neighbors = 10, mean_type = \"equal_weight\"):\n",
    "        # initialize parameters\n",
    "        self.n_neighbors = n_neighbors\n",
    "        if n_neighbors < 10:\n",
    "            self.mean_type = \"equal_weight\"\n",
    "        else:\n",
    "            self.mean_type = mean_type\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_pred = np.zeros(len(x_test))\n",
    "        for i in range(len(x_test)):\n",
    "            # compute the distance\n",
    "            dist = np.zeros(len(self.x))\n",
    "            for j in range(len(self.x)):\n",
    "                dist[j] = np.sum((x_test[i] - self.x[j]) ** 2)\n",
    "\n",
    "            # find k nearnest neighbors\n",
    "            min_dist = np.sort(dist)\n",
    "            min_dist_list = list(map(list(dist).index, min_dist[:self.n_neighbors]))\n",
    "\n",
    "            # find out y according to the values of k nearnest neighbors' distances\n",
    "            y_a = np.zeros(len(min_dist_list))\n",
    "            for j in range(len(min_dist_list)):\n",
    "                y_a[j] = self.y[min_dist_list[j]]\n",
    "\n",
    "            # remove outliers\n",
    "            if self.mean_type == \"remove_outliers\":\n",
    "                Q1 = np.quantile(y_a, 0.25)\n",
    "                Q3 = np.quantile(y_a, 0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                y_a = y_a[(y_a >= (Q1 - 1.5 * IQR)) & (y_a <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "            # compute the mean\n",
    "            y_pred[i] = np.sum(y_a) / len(y_a)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2 & Q1.3\n",
    "以下為我的實作方法，首先先讀取data並對其做標準化，接著分別按照equal_weight和remove_outliers兩種case做prediction，最後再計算RMSE並印出RMSE和prediction的前20項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neighbors = 20, mean_type = equal_weight:\n",
      "RMSE =  10.25126451549596\n",
      "first 20 prediction = \n",
      " [1993.35 1993.8  2000.65 1991.5  1992.8  1998.5  1988.1  1991.65 2002.25\n",
      " 2003.   2000.5  1998.65 1995.55 1997.2  1995.05 1997.4  1992.15 2000.45\n",
      " 2003.2  1995.75]\n",
      "\n",
      "n_neighbors = 20, mean_type = remove_outliers:\n",
      "RMSE =  10.212572466080376\n",
      "first 20 prediction = \n",
      " [1993.35       1993.8        2000.65       1992.73684211 1992.8\n",
      " 2000.         1988.1        1991.65       2002.25       2003.94736842\n",
      " 2000.5        2000.94444444 1995.55       1997.2        1998.61111111\n",
      " 1997.4        1992.15       2003.83333333 2003.2        1995.75      ]\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']\n",
    "\n",
    "myknn = myknn_regressor(20, \"equal_weight\")\n",
    "myknn.fit(X_train, Y_train)\n",
    "ypred = myknn.predict(X_test)\n",
    "RMSE = np.sqrt(np.sum((ypred - Y_test) ** 2) / len(ypred))\n",
    "\n",
    "print(\"n_neighbors = 20, mean_type = equal_weight:\")\n",
    "print(\"RMSE = \", RMSE)\n",
    "print(\"first 20 prediction = \\n\", ypred[:20])\n",
    "\n",
    "print()\n",
    "\n",
    "myknn = myknn_regressor(20, \"remove_outliers\")\n",
    "myknn.fit(X_train, Y_train)\n",
    "ypred = myknn.predict(X_test)\n",
    "RMSE = np.sqrt(np.sum((ypred - Y_test) ** 2) / len(ypred))\n",
    "\n",
    "print(\"n_neighbors = 20, mean_type = remove_outliers:\")\n",
    "print(\"RMSE = \", RMSE)\n",
    "print(\"first 20 prediction = \\n\", ypred[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二題 [Tuning the Hyper-parameter]\n",
    "以下為我實作的方法，分別依照三種cases利用迴圈對不同的$k$值做prediction，然後將結果儲存在list裡，最後再利用matplotlib將折線圖畫出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "# standardize feature values\n",
    "X_train_sd = xscaler.transform(msd_data['X_train'])\n",
    "X_test_sd = xscaler.transform(msd_data['X_test'])\n",
    "\n",
    "X_train = msd_data['X_train']\n",
    "X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']\n",
    "\n",
    "k = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "RMSE_1 = []\n",
    "RMSE_2 = []\n",
    "RMSE_3 = []\n",
    "\n",
    "# predict and calculate rmse for three different cases\n",
    "for i in range(len(k)):\n",
    "    knn_1 = KNeighborsRegressor(n_neighbors=k[i])\n",
    "    knn_1.fit(X_train_sd, Y_train)\n",
    "    ypred_1 = knn_1.predict(X_test_sd)\n",
    "    RMSE_1.append(np.sqrt(np.sum((ypred_1 - Y_test) ** 2) / len(ypred_1)))\n",
    "\n",
    "    knn_2 = KNeighborsRegressor(n_neighbors=k[i])\n",
    "    knn_2.fit(X_train, Y_train)\n",
    "    ypred_2 = knn_2.predict(X_test)\n",
    "    RMSE_2.append(np.sqrt(np.sum((ypred_2 - Y_test) ** 2) / len(ypred_2)))\n",
    "    \n",
    "    myknn = myknn_regressor(k[i], \"remove_outliers\")\n",
    "    myknn.fit(X_train_sd, Y_train)\n",
    "    ypred_3 = myknn.predict(X_test_sd)\n",
    "    RMSE_3.append(np.sqrt(np.sum((ypred_3 - Y_test) ** 2) / len(ypred_3)))\n",
    "\n",
    "# plot the curves of three different cases\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(k, RMSE_1,'o-',color = 'r', label = 'first case')\n",
    "plt.plot(k, RMSE_2,'o-',color = 'g', label = 'second case')\n",
    "plt.plot(k, RMSE_3,'o-',color = 'b', label = 'third case')\n",
    "plt.xlabel(\"k\", fontsize=20, labelpad = 15)\n",
    "plt.ylabel(\"RMSE\", fontsize=20, labelpad = 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下為我得到的結果，其中紅色為case 1，綠色為case 2，藍色為case 3，透過圖片可以觀察出case 2的結果比起另外兩者差上不少，其主要原因是標準化的有無，由於資料中每個不同的feature都是以數值來表示的，然而他們的數值範圍和分布狀況等等並不完全相同，因此進行標準化能讓資料都落在一定範圍內，對於做prediction有很大的幫助，至於case 1和case 3，當$k<10$時是完全重疊的，因為實作myknn_regressor時有加入限制，當$k<10$時mean_type只能為equal weight，而當$k=10$時，case 1的結果比case 3來的好，原因可能是remove outliers後取的neighbors數量不夠多，導致預測結果不夠好，但是當$k$足夠大時，很明顯可以觀察到remove outliers後得到的結果是最好的，因此移除一些比較極端的值後再取平均對於prediction是有幫助的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Lasso Regression]\n",
    "#### Q3.1 基於本題給的$L$，推導新的Coordinate Descent with Soft Thresholding公式\n",
    "$$L = \\frac{1}{2n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T \\mathbf{x}_i - w_0)^2 + \\lambda [\\sum_{j=1}^{M} |w_j|]$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{∂L}{∂w_j} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T \\mathbf{x}_i - w_0)(-x_{i,j}) + \\lambda sgn(w_j) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0 - w_j x_{i,j})(-x_{i,j}) + \\lambda sgn(w_j) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{1}{n} [ \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0)(-x_{i,j}) + w_j \\sum_{i=1}^n (x_{i,j})^2 ] + \\lambda sgn(w_j) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "$$w_j = \\frac{[ \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0) x_{i,j} - n \\lambda sgn(w_j) ]}{\\sum_{i=1}^n(x_{i,j})^2}$$\n",
    "<br>\n",
    "<br>\n",
    "$$w_j^* = \\frac{[ \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0) x_{i,j} ]}{\\sum_{i=1}^n(x_{i,j})^2}$$\n",
    "<br>\n",
    "<br>\n",
    "$$w_j = \\begin{cases} w_j^* - \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2}, & \\text {if $w_j^* - \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2} > 0$} \\\\ w_j^* + \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2}, & \\text {if $w_j^* + \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2} < 0$} \\\\ 0, & \\text {otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.2 使用給定個骨架建構你的mylasso\n",
    "以下為我實作的mylasso架構，其中主要分為三個函式:\n",
    "* init( )是用來定義mylasso這個class的參數\n",
    "* fit( )是主要用來做training的函式，首先依照keep_traindata來決定是否儲存資料，接著先利用ridge regression來計算出起始的$w$，接著就按照上面所推導出的公式來進行coordinate descent with soft thresholding直到收斂或完成所有iteration，而我也有把$w_0$也就是bias term拿來做coordinate descent\n",
    "* predict( )是將讀入的testing data和用fit( )得到的$w$來做prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class mylasso():\n",
    "    def __init__(self, lamcoef = 0.1, max_iter=1000, tol=1e-6, const_regu = False):\n",
    "        # initialize parameters\n",
    "        self.lamcoef = lamcoef\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.const_regu = const_regu\n",
    " \n",
    "    def fit(self, x_train, y_train, winit = \"ridge\", keep_traindata = True, verbose = False):\n",
    "        if keep_traindata:\n",
    "          self.x_train = x_train\n",
    "\n",
    "        # calculate an initial w with Ridge Regression\n",
    "        w = np.dot(np.linalg.inv(0.1 * np.identity(x_train.shape[1]) + np.dot(x_train.transpose(), x_train)), np.dot(x_train.transpose(), y_train))\n",
    "        \n",
    "        # calculate the loss function L or L'\n",
    "        if self.const_regu:\n",
    "            L = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(w))\n",
    "        else:\n",
    "            L = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(np.delete(w, 0, 0)))\n",
    "\n",
    "        self.w = w\n",
    "        L_min = L\n",
    "\n",
    "        for i in range(self.max_iter):    \n",
    "            # implement coordinate descent with soft thresholding\n",
    "            for j in range(len(w)):\n",
    "                w_star = np.sum((y_train - np.dot(np.delete(x_train, j, 1), np.delete(w, j, 0))) * x_train[:, j]) / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j]))\n",
    "                if w_star - len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j])) > 0:\n",
    "                    w[j] = w_star - len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j]))\n",
    "                elif w_star + len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j])) < 0:\n",
    "                    w[j] = w_star + len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j]))\n",
    "                else:\n",
    "                    w[j] = 0\n",
    "            \n",
    "            # calculate the loss after an iteration\n",
    "            if self.const_regu:\n",
    "                L_new = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(w))\n",
    "            else:\n",
    "                L_new = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(np.delete(w, 0, 0)))\n",
    "\n",
    "            # update w with the lowest loss\n",
    "            if L_new < L_min:\n",
    "                self.w = w\n",
    "                L_min = L_new\n",
    "\n",
    "            # compare the loss before and after an iteration\n",
    "            if np.abs(L - L_new) < self.tol:\n",
    "                break\n",
    "\n",
    "            L = L_new\n",
    "\n",
    "        # calculate RMSE\n",
    "        ypred = np.dot(x_train, self.w)\n",
    "        RMSE = np.sqrt(np.sum((ypred - y_train) ** 2) / len(ypred))\n",
    "\n",
    "        # calculate MAE\n",
    "        MAE = np.sum(np.abs(ypred - y_train)) / len(ypred)\n",
    "\n",
    "        # calculate number of nonzero weights\n",
    "        nonzero_w = 0\n",
    "        for i in range(len(self.w)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if self.w[i] != 0:\n",
    "                nonzero_w += 1\n",
    "\n",
    "        print(\"Training Loss = \", L_min)\n",
    "        print(\"Train RMSE = \", RMSE)\n",
    "        print(\"Train MAE = \", MAE)\n",
    "        print(\"Intercept = \", self.w[0])\n",
    "        print(\"Feature Weights = \\n\", self.w[1:])\n",
    "        print(\"Number of Nonzero Weights = \", nonzero_w)\n",
    " \n",
    "    def predict(self, x_test):\n",
    "        y_pred = np.dot(x_test, self.w)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.3 & Q3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下為我實作的方法，先讀取資料並標準化，接著將training data傳進mylasso做training，由於我在上面架構中的fit函式裡有加入印出training loss, RMSE, MAE, intercept, feature weights, number of nonzero weights這些資訊，因此執行fit( )就會自動印出，最後只要再印出prediction的結果和計算出的RMSE和MAE即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss =  49.893404109953586\n",
      "Train RMSE =  9.720013505274403\n",
      "Train MAE =  7.003889367216193\n",
      "Intercept =  1998.1660000000002\n",
      "Feature Weights = \n",
      " [ 4.6518551  -2.45673153 -1.06195885  0.         -0.34275054 -2.94854329\n",
      "  0.         -0.39970268 -0.42653866  0.         -0.23498651 -0.39491058\n",
      "  0.50307719  0.         -0.34207893  0.61674589  0.15722753  0.43489925\n",
      "  0.43595375  1.12941661  0.40663481  0.          1.77743607  0.\n",
      " -0.16842728  0.07833529  0.66391172  0.01297938  0.15350723  0.\n",
      " -0.21762074 -0.22189107  0.         -0.0027548  -0.01707674 -0.2599464\n",
      "  0.          0.28969362  0.34893387  0.         -0.26112569 -0.14488702\n",
      " -0.00995271  0.03497723 -0.03984338  0.          0.00299383 -0.17804306\n",
      "  0.          0.06494382  0.27076506  0.          0.          0.\n",
      "  0.          0.         -0.63813223  0.23761428 -0.19016387  0.\n",
      " -0.18875525  0.         -0.13590668  0.0612754  -0.26619017  0.\n",
      "  0.          0.         -0.03099184  0.         -0.19276517  0.01221358\n",
      "  0.20735199  0.25118247  0.12313494  0.          0.         -0.52945662\n",
      "  0.          0.          0.17859187  0.02408561  0.25291058  0.19360205\n",
      "  0.37064854  0.          0.         -0.29169753  0.          0.        ]\n",
      "Number of Nonzero Weights =  59\n",
      "\n",
      "first 5 prediction =  [1991.83191145 1998.97198637 2001.66293602 1991.20317029 1994.32657933]\n",
      "Test RMSE =  9.650076299177924\n",
      "Test MAE =  6.943728810915706\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    " \n",
    "# Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "# standardize feature values\n",
    "X_train_sd = xscaler.transform(msd_data['X_train'])\n",
    "X_test_sd = xscaler.transform(msd_data['X_test'])\n",
    "\n",
    "# add a column with all ones\n",
    "const_train = np.ones((X_train_sd.shape[0], 1))\n",
    "X_train_sd = np.concatenate((const_train, X_train_sd), axis=1)\n",
    "const_test = np.ones((X_test_sd.shape[0], 1))\n",
    "X_test_sd = np.concatenate((const_test, X_test_sd), axis=1)\n",
    " \n",
    "# outcome values\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']\n",
    "\n",
    "mlo = mylasso(lamcoef = 0.1)\n",
    "mlo.fit(X_train_sd, Y_train)\n",
    "ypred = mlo.predict(X_test_sd)\n",
    "\n",
    "RMSE = np.sqrt(np.sum((ypred - Y_test) ** 2) / len(ypred))\n",
    "MAE = np.sum(np.abs(ypred - Y_test)) / len(ypred)\n",
    "print()\n",
    "print(\"first 5 prediction = \", ypred[:5])\n",
    "print(\"Test RMSE = \", RMSE)\n",
    "print(\"Test MAE = \", MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
