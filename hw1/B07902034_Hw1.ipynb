{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習 (Fall, 2020)\n",
    "### Homework 1 \n",
    "B07902034 資工三 王昱凱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一題 [myknn_regressor]\n",
    "#### Q1.1 Create your myknn_regressor based on the skeleton.\n",
    "\n",
    "以下為我實作的myknn_regressor架構，其中主要分為三個函式:\n",
    "* init( ) 是用來定義這個class會使用到的參數，比較特別的是我會先判斷n_neighbors是否大於10，接著才對mean_type做定義\n",
    "* fit( ) 是用來定義接下來prediction會使用到的data (x_train & y_train)\n",
    "* predict( ) 是主要實作knn的函式，首先先計算跟所有neighbors之間的距離，接著排序並找出前$k$小的值，再來找出對應的$y_a$，如果mean_type是remove_outliers的話，則計算$y_a$的第一四分位數和第三四分位數並且將$y_a$的範圍限制在$[Q1 - 1.5 IQR, Q3 + 1.5 IQR]$，最後計算$y_a$的平均即為結果\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "class myknn_regressor():\n",
    "    def __init__(self, n_neighbors = 10, mean_type = \"equal_weight\"):\n",
    "        # initialize parameters\n",
    "        self.n_neighbors = n_neighbors\n",
    "        if n_neighbors < 10:\n",
    "            self.mean_type = \"equal_weight\"\n",
    "        else:\n",
    "            self.mean_type = mean_type\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_pred = np.zeros(len(x_test))\n",
    "        for i in range(len(x_test)):\n",
    "            # calculate the distance\n",
    "            dist = np.zeros(len(self.x))\n",
    "            for j in range(len(self.x)):\n",
    "                dist[j] = np.sum((x_test[i] - self.x[j]) ** 2)\n",
    "\n",
    "            # find k nearnest neighbors\n",
    "            min_dist = np.sort(dist)\n",
    "            min_dist_list = list(map(list(dist).index, min_dist[:self.n_neighbors]))\n",
    "\n",
    "            # find out y according to the values of k nearnest neighbors' distances\n",
    "            y_a = np.zeros(len(min_dist_list))\n",
    "            for j in range(len(min_dist_list)):\n",
    "                y_a[j] = self.y[min_dist_list[j]]\n",
    "\n",
    "            # remove outliers\n",
    "            if self.mean_type == \"remove_outliers\":\n",
    "                Q1 = np.quantile(y_a, 0.25)\n",
    "                Q3 = np.quantile(y_a, 0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                y_a = y_a[(y_a >= (Q1 - 1.5 * IQR)) & (y_a <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "            # calculate the mean\n",
    "            y_pred[i] = np.sum(y_a) / len(y_a)\n",
    "        return y_pred\n",
    "```\n",
    "\n",
    "#### Q1.2 & Q1.3\n",
    "以下為我的實作方法，首先先讀取data並對其做標準化，接著分別按照equal_weight和remove_outliers兩種case做prediction，最後再計算RMSE並印出RMSE和prediction的前20項\n",
    "\n",
    "```python\n",
    "#Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']\n",
    "\n",
    "# make prediction\n",
    "\n",
    "# myknn = myknn_regressor(20, \"equal_weight\")\n",
    "myknn = myknn_regressor(20, \"remove_outliers\")\n",
    "myknn.fit(X_train, Y_train)\n",
    "ypred = myknn.predict(X_test)\n",
    "\n",
    "# calculate root mean square error\n",
    "RMSE = 0\n",
    "for i in range(len(ypred)):\n",
    "    RMSE += (ypred[i] - Y_test[i]) ** 2\n",
    "RMSE = np.sqrt(RMSE / len(ypred))\n",
    "\n",
    "print(\"RMSE = \", RMSE)\n",
    "print(\"first 20 prediction = \\n\", ypred[:20])\n",
    "```\n",
    "\n",
    "n_neighbors = 20, mean_type = \"equal_weight\":\n",
    "\n",
    "n_neighbors = 20, mean_type = \"remove_outliers\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二題 [Tuning the Hyper-parameter]\n",
    "以下為我實作的方法，分別依照三種cases利用迴圈對不同的$k$值做prediction，然後將結果儲存在list裡，最後再利用matplotlib將折線圖畫出來\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from knn import myknn_regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# compute root mean square\n",
    "def compute_rmse(ypred, Y_test):\n",
    "    RMSE = 0\n",
    "    for i in range(len(ypred)):\n",
    "        RMSE += (ypred[i] - Y_test[i]) ** 2\n",
    "    RMSE = math.sqrt(RMSE / len(ypred))\n",
    "    return RMSE\n",
    "\n",
    "# Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "# standardize feature values\n",
    "X_train_sd = xscaler.transform(msd_data['X_train'])\n",
    "X_test_sd = xscaler.transform(msd_data['X_test'])\n",
    "\n",
    "X_train = msd_data['X_train']\n",
    "X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']\n",
    "\n",
    "k = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "RMSE_1 = []\n",
    "RMSE_2 = []\n",
    "RMSE_3 = []\n",
    "\n",
    "# predict and calculate rmse for three different cases\n",
    "for i in range(len(k)):\n",
    "    print(i)\n",
    "    knn_1 = KNeighborsRegressor(n_neighbors=k[i])\n",
    "    knn_1.fit(X_train_sd, Y_train)\n",
    "    ypred_1 = knn_1.predict(X_test_sd)\n",
    "    RMSE_1.append(compute_rmse(ypred_1, Y_test))\n",
    "\n",
    "    knn_2 = KNeighborsRegressor(n_neighbors=k[i])\n",
    "    knn_2.fit(X_train, Y_train)\n",
    "    ypred_2 = knn_2.predict(X_test)\n",
    "    RMSE_2.append(compute_rmse(ypred_2, Y_test))\n",
    "    \n",
    "    myknn = myknn_regressor(k[i], \"remove_outliers\")\n",
    "    myknn.fit(X_train_sd, Y_train)\n",
    "    ypred_3 = myknn.predict(X_test_sd)\n",
    "    RMSE_3.append(compute_rmse(ypred_3, Y_test))\n",
    "\n",
    "# plot the curves of three different cases\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(k, RMSE_1,'o-',color = 'r', label = 'first case')\n",
    "plt.plot(k, RMSE_2,'o-',color = 'g', label = 'second case')\n",
    "plt.plot(k, RMSE_3,'o-',color = 'b', label = 'third case')\n",
    "plt.xlabel(\"k\", fontsize=30, labelpad = 15)\n",
    "plt.ylabel(\"RMSE\", fontsize=30, labelpad = 15)\n",
    "plt.savefig(\"tuning.png\")\n",
    "```\n",
    "\n",
    "以下為我得到的結果，其中紅色為case 1，綠色為case 2，藍色為case 3，透過圖片可以觀察出case 2的結果比起另外兩者差上不少，其主要原因是標準化的有無，由於資料中每個不同的feature都是以數值來表示的，然而他們的數值範圍和分布狀況等等並不完全相同，因此進行標準化能讓資料都落在一定範圍內，對於做prediction有很大的幫助，至於case 1和case 3，當$k<10$時是完全重疊的，因為實作myknn_regressor時有加入限制，當$k<10$時mean_type只能為equal weight，而當$k=10$時，case 1的結果比case 3來的好，原因可能是remove outliers後取的neighbors數量不夠多，導致預測結果不夠好，但是當$k$足夠大時，很明顯可以觀察到remove outliers後得到的結果是最好的，因此移除一些比較極端的值後再取平均對於prediction是有幫助的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Lasso Regression]\n",
    "#### Q3.1 基於本題給的$L$，推導新的Coordinate Descent with Soft Thresholding公式\n",
    "$$L = \\frac{1}{2n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T \\mathbf{x}_i - w_0)^2 + \\lambda [\\sum_{j=1}^{M} |w_j|]$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{∂L}{∂w_j} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T \\mathbf{x}_i - w_0)(-x_{i,j}) + \\lambda sgn(w_j) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0 - w_j x_{i,j})(-x_{i,j}) + \\lambda sgn(w_j) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{1}{n} [ \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0)(-x_{i,j}) + w_j \\sum_{i=1}^n (x_{i,j})^2 ] + \\lambda sgn(w_j) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "$$w_j = \\frac{[ \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0) x_{i,j} - n \\lambda sgn(w_j) ]}{\\sum_{i=1}^n(x_{i,j})^2}$$\n",
    "<br>\n",
    "<br>\n",
    "$$w_j^* = \\frac{[ \\sum_{i=1}^n (y_i - \\mathbf{w}^T_{-j}  \\mathbf{x}_{i,-j} - w_0) x_{i,j} ]}{\\sum_{i=1}^n(x_{i,j})^2}$$\n",
    "<br>\n",
    "<br>\n",
    "$$w_j = \\begin{cases} w_j^* - \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2}, & \\text {if $w_j^* - \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2} > 0$} \\\\ w_j^* + \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2}, & \\text {if $w_j^* + \\frac{n \\lambda}{\\sum_{i=1}^n(x_{i,j})^2} < 0$} \\\\ 0, & \\text {otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.2 使用給定個骨架建構你的mylasso\n",
    "以下為我實作的mylasso架構，其中主要分為三個函式:\n",
    "* init( )是用來定義mylasso這個class的參數\n",
    "* fit( )是主要用來做training的函式，首先依照keep_traindata來決定是否儲存資料，接著先利用ridge regression來計算出起始的$w$，接著就按照上面所推導出的公式來進行coordinate descent with soft thresholding直到收斂或完成所有iteration\n",
    "* predict( )是將讀入的testing data和用fit( )得到的$w$來做prediction\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class mylasso():\n",
    "    def __init__(self, lamcoef = 0.1, max_iter=1000, tol=1e-6, const_regu = False):\n",
    "        # initialize parameters\n",
    "        self.lamcoef = lamcoef\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.const_regu = const_regu\n",
    " \n",
    "    def fit(self, x_train, y_train, winit = \"ridge\", keep_traindata = True, verbose = False):\n",
    "        if keep_traindata:\n",
    "          self.x_train = x_train\n",
    "\n",
    "        # calculate an initial w with Ridge Regression\n",
    "        w = np.dot(np.linalg.inv(0.1 * np.identity(x_train.shape[1]) + np.dot(x_train.transpose(), x_train)), np.dot(x_train.transpose(), y_train))\n",
    "        \n",
    "        # calculate the loss function L or L'\n",
    "        if self.const_regu:\n",
    "            L = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(w))\n",
    "        else:\n",
    "            L = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(np.delete(w, 0, 0)))\n",
    "\n",
    "        self.w = w\n",
    "        L_min = L\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            if verbose:\n",
    "                print(\"the \" + str(i) + \" iteration, the loss = \" + str(L))      \n",
    "            # implement coordinate descent with soft thresholding\n",
    "            for j in range(len(w)):\n",
    "                w_star = np.sum((y_train - np.dot(np.delete(x_train, j, 1), np.delete(w, j, 0))) * x_train[:, j]) / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j]))\n",
    "                if w_star - len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j])) > 0:\n",
    "                    w[j] = w_star - len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j]))\n",
    "                elif w_star + len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j])) < 0:\n",
    "                    w[j] = w_star + len(y_train) * self.lamcoef / np.sum(np.dot(x_train[:, j].transpose(), x_train[:, j]))\n",
    "                else:\n",
    "                    w[j] = 0\n",
    "            \n",
    "            # calculate the loss after an iteration\n",
    "            if self.const_regu:\n",
    "                L_new = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(w))\n",
    "            else:\n",
    "                L_new = np.sum(np.power(y_train - np.dot(x_train, w), 2))/2/len(y_train) + self.lamcoef*np.sum(np.abs(np.delete(w, 0, 0)))\n",
    "\n",
    "            # update w with the lowest loss\n",
    "            if L_new < L_min:\n",
    "                self.w = w\n",
    "                L_min = L_new\n",
    "\n",
    "            # compare the loss before and after an iteration\n",
    "            if np.abs(L - L_new) < self.tol:\n",
    "                break\n",
    "\n",
    "            L = L_new\n",
    " \n",
    "    def predict(self, x_test):\n",
    "        y_pred = np.dot(x_test, self.w)\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
