{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習\n",
    "### Homework 2\n",
    "B07902034 資工三 王昱凱\n",
    "### 第一題 [Data Preprocessing]\n",
    "此題我們需要對從UCI上下載的dataset做preprocessing，而對於實作preprocessing，我大致上分為三個步驟：\n",
    "* 先從檔案裡逐行將資料讀出並且以逗號分隔開存入nparray裡，接著將有缺值的資料移除，而y為資料中的最後一個feature，將>50K和<=50K轉換為1和0的label\n",
    "* 為了做1-of-K encoding，需要先對連續性資料中的每個field的每種feature做統計，接著將出現次數不到10次的feature移除，移除後我個別對每個field中的features按照字母做排序\n",
    "* 將統計好的feature按照助教所提供的順序將每個field做合併，最後再將每筆資料按照建立好的1-of-K encoding格式讀入，並且讀完後以訓練資料的平均和標準差做標準化即可完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# load the data by lines and split them with \", \"\n",
    "with open('adult.data') as f:\n",
    "    X_train = np.array([line.strip('\\n').split(', ')[:-1] for line in f])\n",
    "with open('adult.data') as f:\n",
    "    Y_train = np.array([line.strip('\\n').split(', ')[-1] for line in f])\n",
    "with open('adult.test') as f:\n",
    "    next(f)\n",
    "    X_test = np.array([line.strip('\\n').split(', ')[:-1] for line in f])\n",
    "with open('adult.test') as f:\n",
    "    next(f)\n",
    "    Y_test = np.array([line.strip('\\n').split(', ')[-1] for line in f])\n",
    "    \n",
    "# the last line in the file is an empty string\n",
    "X_train = X_train[:-1]\n",
    "Y_train = Y_train[:-1]\n",
    "X_test = X_test[:-1]\n",
    "Y_test = Y_test[:-1]\n",
    "\n",
    "# remove the row with missing value\n",
    "i = 0\n",
    "while i < len(X_train):\n",
    "    if '?' in X_train[i]:\n",
    "        X_train = np.delete(X_train, i, 0)\n",
    "        Y_train = np.delete(Y_train, i)\n",
    "        i = i-1\n",
    "    i = i+1\n",
    "    \n",
    "i = 0\n",
    "while i < len(X_test):\n",
    "    if '?' in X_test[i]:\n",
    "        X_test = np.delete(X_test, i, 0)\n",
    "        Y_test = np.delete(Y_test, i)\n",
    "        i = i-1\n",
    "    i = i+1\n",
    "\n",
    "# set y_train and y_test to binary-valued\n",
    "y_train = np.array([1 if y == '>50K' else 0 for y in Y_train])\n",
    "y_test = np.array([1 if y == '>50K.' else 0 for y in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary-valued data\n",
    "workclass, workclass_cnt = [], []\n",
    "education, education_cnt = [], []\n",
    "marital, marital_cnt = [], []\n",
    "ocupation, ocupation_cnt = [], []\n",
    "relationship, relationship_cnt = [], []\n",
    "race, race_cnt = [], []\n",
    "sex, sex_cnt = [], []\n",
    "country, country_cnt = [], []\n",
    "feature = [[workclass, workclass_cnt], [education, education_cnt], [marital, marital_cnt], \n",
    "           [ocupation, ocupation_cnt], [relationship, relationship_cnt], [race, race_cnt], \n",
    "           [sex, sex_cnt], [country, country_cnt]]\n",
    "\n",
    "# index of binary-valued data in dataset on UCI\n",
    "feature_index = [1, 3, 5, 6, 7, 8, 9, 13]\n",
    "\n",
    "# count the appearances of each features in each fields\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(feature_index)):\n",
    "        if X_train[i][feature_index[j]] not in feature[j][0]:\n",
    "            feature[j][0].append(X_train[i][feature_index[j]])\n",
    "            feature[j][1].append(1)\n",
    "        else:\n",
    "            index = feature[j][0].index(X_train[i][feature_index[j]])\n",
    "            feature[j][1][index] += 1\n",
    "\n",
    "# delete the features with less than 10 appearances and sort them according to alphabets\n",
    "for i in range(len(feature)):\n",
    "    j = 0\n",
    "    while j < len(feature[i][0]):\n",
    "        if feature[i][1][j] < 10:\n",
    "            del feature[i][0][j]\n",
    "            del feature[i][1][j]\n",
    "            j -= 1\n",
    "        j += 1\n",
    "    feature[i][0] = sorted(feature[i][0])\n",
    "\n",
    "# continuous-valued data\n",
    "feature_cont = ['capital-loss', 'hours-per-week', 'capital-gain', 'educational-num', 'age', 'fnlwgt']\n",
    "columnname = feature_cont\n",
    "\n",
    "# concatenate different fields into the same order as adult50kp['columnname'] \n",
    "feature_index = [4, 5, 6, 3, 1, 7, 0, 2]\n",
    "for i in feature_index:\n",
    "    columnname += feature[i][0]\n",
    "\n",
    "# construct x_train and x_test by one-of-K encoding\n",
    "feature_cont_index = [11, 12, 10, 4, 0, 2]\n",
    "x_train = np.zeros((len(X_train), len(columnname)))\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(feature_cont_index)):\n",
    "        x_train[i][j] = X_train[i][feature_cont_index[j]]\n",
    "    for j in range(len(X_train[i])):\n",
    "        if j not in feature_cont_index and X_train[i][j] in columnname:\n",
    "            index = columnname.index(X_train[i][j])\n",
    "            x_train[i][index] = 1        \n",
    "\n",
    "x_test = np.zeros((len(X_test), len(columnname)))\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(feature_cont_index)):\n",
    "        x_test[i][j] = X_test[i][feature_cont_index[j]]\n",
    "    for j in range(len(X_test[i])):\n",
    "        if j not in feature_cont_index and X_test[i][j] in columnname:\n",
    "            index = columnname.index(X_test[i][j])\n",
    "            x_test[i][index] = 1\n",
    "\n",
    "# standardize x_train and x_test with x_train's mean and std\n",
    "for i in range(6):\n",
    "    mean = np.mean(x_train[:, i])\n",
    "    std = np.std(x_train[:, i])\n",
    "    x_train[:, i] = (x_train[:, i] - mean) / std\n",
    "    x_test[:, i] = (x_test[:, i] - mean) / std\n",
    "\n",
    "adult50k = {'x_train':x_train, 'x_test':x_test, 'y_train':y_train, 'y_test':y_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train match!\n",
      "x_test match!\n",
      "y_train match!\n",
      "y_test match!\n"
     ]
    }
   ],
   "source": [
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "elems = ['x_train', 'x_test', 'y_train', 'y_test']\n",
    "\n",
    "for aelem in elems:\n",
    "    cnomatch = np.sum(adult50kp[aelem] != adult50k[aelem])\n",
    "    if cnomatch == 0:\n",
    "        print(aelem, \"match!\")\n",
    "    else:\n",
    "        print(aelem, \"%d elements no match!\" % cnomatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二題 [ROC and AUC]\n",
    "#### Q2.1 基於adult50kp['y_test']與ypredprob繪製ROC Curve\n",
    "以下為我的實作方式，首先對從sklearn的logistic regression model預測出來的機率做排序，由於預測資料總共有約15000筆，我取了約3000個thresholds，也就是每5筆資料取一個threshold，接著對這約3000個thresholds分別計算fpr和tpr，最後再將計算出的約3000組fpr和tpr繪製成roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.848340\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load dataset\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "#train prediction model    \n",
    "c = 0.3\n",
    "lr2 = LogisticRegression(solver = 'lbfgs', C= c, max_iter = 1000)\n",
    "lr2.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "#make prediction\n",
    "ypred = lr2.predict(adult50kp['x_test'])\n",
    "ypredprob = lr2.predict_proba(adult50kp['x_test'])\n",
    "#compute accuracy\n",
    "ncorrect = np.sum(adult50kp['y_test'] == ypred)\n",
    "accuracy_sk = ncorrect / adult50kp['y_test'].shape[0]\n",
    "print(\"Accuracy = %f\" % accuracy_sk) \n",
    "\n",
    "# decide thresholds by sorted ypredprob\n",
    "thresholds = np.sort(ypredprob[:, 1])[::-1]\n",
    "thresholds = np.append(thresholds[0]+1, thresholds)\n",
    "index = []\n",
    "for i in range(len(thresholds)):\n",
    "    if i % 5 != 0:\n",
    "        index.append(i)\n",
    "thresholds = np.delete(thresholds, index)\n",
    "\n",
    "# compute fpr & tpr\n",
    "fpr, tpr = [], []\n",
    "for i in range(len(thresholds)):\n",
    "    tp, fn, fp, tn = 0, 0, 0, 0\n",
    "    for j in range(len(adult50kp['y_test'])):\n",
    "        if adult50kp['y_test'][j] == 1:\n",
    "            if ypredprob[j, 1] >= thresholds[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if ypredprob[j, 1] >= thresholds[i]:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    fpr.append(fp/(fp+tn))\n",
    "    tpr.append(tp/(tp+fn))\n",
    "    \n",
    "# plot the roc curve\n",
    "plt.plot(fpr, tpr, color='r')\n",
    "plt.plot([0, 1], [0, 1], color='b')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.2 計算繪製出的ROC Curve的AUC\n",
    "按照助教所提供的方法，以計算梯形的方式來當作曲線面積的近似值，因此以每個threshold作為間隔，將計算出的梯形面積累加即可得到AUC的近似值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC =  0.9034924462314518\n"
     ]
    }
   ],
   "source": [
    "# compute auc base on roc curve\n",
    "auc = 0\n",
    "for i in range(len(thresholds)-1):\n",
    "    auc += (tpr[i+1] + tpr[i]) * (fpr[i+1] - fpr[i]) / 2\n",
    "print(\"AUC = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Logistic Regression with L2 Regularization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1 Derive the gradient and hessian matrix for the new E(w)\n",
    "$$E(w) = \\frac{1}{2} w^T \\Lambda w - \\sum_{n=1}^N [ t_n \\ln y_n  + (1 - t_n) \\ln (1 - y_n)]\\ ,\\ where\\ y_n = \\frac{1}{1 + exp({-w^Tx_n})}$$\n",
    "<br>\n",
    "<br>\n",
    "$$Let\\ z_1 = \\sum_{n=1}^N (t_n \\ln y_n)\\ ,\\ z_2 = \\sum_{n=1}^N [(1 - t_n) \\ln (1 - y_n)]\\ , \\ where\\ y_n = \\sigma (w^Tx_n)$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{∂z_1}{∂w} = \\frac{t_n \\sigma(w^Tx_n)(1 - \\sigma(w^Tx_n)x_n}{ \\sigma(w^Tx_n)} = t_n(1 - \\sigma(w^Tx_n)x_n$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{∂z_2}{∂w} = \\frac{(1 - t_n) \\sigma(w^Tx_n)(1 - \\sigma(w^Tx_n)(- x_n)}{1 - \\sigma(w^Tx_n)} = (t_n - 1) \\sigma(w^Tx_n)x_n$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac{∂z_1}{∂w} + \\frac{∂z_2}{∂w} = (t_n - \\sigma(w^Tx_n)x_n$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\nabla E(w) = \\frac{∂E(w)}{∂w} = \\Lambda w - \\sum_{n=1}^N (t_n - \\sigma(w^Tx_n)x_n = \\Lambda w + \\Phi^T(y-t)$$\n",
    "<br>\n",
    "<br>\n",
    "$$Hessian = \\nabla \\nabla E(w) = \\sum_{n=1}^N y_n (1 - y_n) x_n x_n^T + \\Lambda = \\Phi^T R \\Phi + \\Lambda \\ ,\\ where\\ R\\ is\\ a\\ diagonal\\ matrix\\ with\\ R_{nn} = y_n (1 - y_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2 Create your mylogistic_l2 class\n",
    "以下為我實作的mylogistic_l2架構，其中主要分為三個函式:\n",
    "* init() 是用來定義這個class的參數\n",
    "* fit() 是主要用來做training的函式，先按照add_intercept決定是否加入全為1的column，利用ridge regression找出一個初始的weight，接著按照上一小題得出的gradient和hessian來實作newton raphson method，以此更新weight，比較更新weight前後的error來判斷是否收斂，收斂後存下error最低的weight\n",
    "* predict() 是用來做prediction的，將計算後得出的機率以0.5為threshold判斷為label 1或0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogistic_l2():\n",
    "    def __init__(self, reg_vec, max_iter = 100, tol = 1e-5, add_intercept = True):\n",
    "        # initialize the parameters\n",
    "        self.reg_vec = reg_vec\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "\n",
    "    def fit(self, x, y, verbal = False):\n",
    "        # add intercept at the end of x\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((x.shape[0], 1))\n",
    "            x = np.concatenate((x, intercept), axis=1)\n",
    "            \n",
    "        # compute initial w by ridge regression\n",
    "        b = np.sum(self.reg_vec)/len(self.reg_vec)\n",
    "        w = np.dot(np.linalg.inv(b * np.identity(x.shape[1]) + np.dot(x.transpose(), x)), np.dot(x.transpose(), y))\n",
    "        \n",
    "        \n",
    "        # compute the error function E(w)\n",
    "        ypred = np.clip(1 / (1.0 + np.exp(-np.dot(x, w))), 1e-8, 1-(1e-8))\n",
    "        E = np.dot(np.dot(w, self.reg_vec), w)/2 - (np.dot(y, np.log(ypred)) + np.dot((1-y), np.log(1-ypred)))\n",
    "\n",
    "        self.w = w\n",
    "        E_min = E\n",
    "\n",
    "        for i in range(self.max_iter): \n",
    "            # implement logistic regression with newton raphson method\n",
    "            ypred = np.clip(1 / (1.0 + np.exp(-np.dot(x, w))), 1e-8, 1-(1e-8))\n",
    "            gradient =  np.dot(x.transpose(), (ypred - y)) + np.dot(w, self.reg_vec)\n",
    "            R = np.identity(len(ypred))\n",
    "            for j in range(len(ypred)):\n",
    "                R[j][j] = ypred[j] * (1 - ypred[j])\n",
    "            Hessian = np.dot(np.dot(x.transpose(), R), x) + self.reg_vec\n",
    "            w = w - np.dot(np.linalg.inv(Hessian), gradient)\n",
    "            \n",
    "            # compute the error after an iteration\n",
    "            ypred = np.clip(1 / (1.0 + np.exp(-np.dot(x, w))), 1e-8, 1-(1e-8))\n",
    "            E_new = np.dot(np.dot(w, self.reg_vec), w)/2 - (np.dot(y, np.log(ypred)) + np.dot((1-y), np.log(1-ypred)))\n",
    "            \n",
    "            # update w with the lowest error\n",
    "            if E_new < E_min:\n",
    "                self.w = w\n",
    "                E_min = E_new\n",
    "\n",
    "            # compare the error before and after an iteration\n",
    "            if np.abs(E - E_new) < self.tol:\n",
    "                break\n",
    "            \n",
    "            E = E_new\n",
    "            \n",
    "        if verbal:\n",
    "            print(\"w = \\n\", self.w)\n",
    "            \n",
    "            \n",
    "    def predict(self, x):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((x.shape[0], 1))\n",
    "            x = np.concatenate((x, intercept), axis=1)\n",
    "        \n",
    "        # predict with trained weight\n",
    "        ypred = np.clip(1 / (1.0 + np.exp(-np.dot(x, self.w))), 1e-8, 1-(1e-8))\n",
    "        ypred = np.array([1 if y >= 0.5 else 0 for y in ypred])\n",
    "        return ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 1 : \n",
      "w = \n",
      " [ 0.25831075  0.35295138  2.33390165  0.75114521  0.33352443  0.07923687\n",
      " -0.25930599 -0.03310592 -0.80209231 -1.16328376 -0.15748027  1.06974337\n",
      " -0.63384606  0.11673241 -0.23156738 -0.5171222  -0.07972164 -1.0994978\n",
      " -0.24602708  0.06196949  0.12668588  0.86265606 -0.91835285 -0.62122618\n",
      " -0.20074023 -0.75160098 -1.61011588  0.57582091  0.64899529  0.35374143\n",
      "  0.71721848 -0.02844947 -0.00095482 -0.1965409  -0.14635164  0.62694628\n",
      "  0.44820708  0.02459458  0.04692237 -0.49106775 -0.20303543 -0.16330368\n",
      " -0.01766235 -0.11132832 -0.09946183 -1.17391916  0.18070269 -0.069272\n",
      "  0.97649691  0.46098861 -0.49544042 -1.27203531  0.48677241 -0.89896372\n",
      " -0.06005426 -0.35084886  0.43281522  0.59412014  0.58215192 -0.6209623\n",
      " -0.05974804  0.09290353 -0.1518921  -0.00538528  0.0341609  -0.28908824\n",
      "  0.15605391  0.49540124  0.89094226  0.14915144  0.34248478 -0.31331216\n",
      " -0.35593911 -0.3624946  -0.66724747 -0.40883113  0.44748984  0.13776893\n",
      "  0.14135123 -0.11601542 -0.05610327 -0.93458304 -0.02925965 -0.29901296\n",
      " -0.15051125  0.35233187 -0.78584654  0.58020021  0.49704231 -0.19032074\n",
      " -0.00034772  0.17499381 -0.4882027  -0.31225962 -1.02643023 -0.72231083\n",
      "  1.44672469  1.15520745 -0.6802029  -1.2119563  -0.79833851 -0.53464848\n",
      " -1.34552488]\n",
      "test accuracy =  0.847875166002656\n",
      "\n",
      "case 2 : \n",
      "w = \n",
      " [ 0.25833063  0.35307341  2.33348267  0.7378757   0.33385106  0.07926886\n",
      " -0.04219571  0.1998764  -0.58360968 -0.93671312  0.07548467  1.28715745\n",
      " -0.37140328  0.39422898  0.04305748 -0.26147347  0.1955903  -0.42695771\n",
      "  0.42695771  0.16424528  0.22840772  0.96472553 -0.8174378  -0.52074423\n",
      " -0.09910239 -0.64944042 -1.55235099  0.6786798   0.75066429  0.45541098\n",
      "  0.81857112  0.07308911  0.07284641 -0.11752645 -0.06282948  0.67242506\n",
      "  0.5040869   0.0879909   0.11435014 -0.38483985 -0.1019631  -0.05145375\n",
      "  0.10741777 -0.01997934  0.01717544 -1.16567809  0.30082277  0.02715464\n",
      "  1.00831207  0.50210397 -0.45756663 -1.24002555  0.52780939 -0.86832687\n",
      " -0.02771494 -0.31412702  0.47343435  0.6298111   0.62405658 -0.58675062\n",
      " -0.0296708   0.12414401 -0.14376238  0.02434194  0.06216039 -0.24843986\n",
      "  0.19459429  0.52620501  0.93165615  0.18707696  0.37950109 -0.28749402\n",
      " -0.31137357 -0.33290534 -0.65117786 -0.38160105  0.48879121  0.17662205\n",
      "  0.17410342 -0.07343503 -0.0314651  -0.89846776  0.00653561 -0.27232554\n",
      " -0.12442075  0.39697177 -0.75318728  0.61067658  0.70544004  0.01789988\n",
      "  0.2090388   0.382747   -0.2795817  -0.10453082 -0.9310132  -0.52642474\n",
      "  1.61398954  1.36735898 -0.49235221 -1.01493649 -0.60567591 -0.34195917\n",
      " -3.17508577]\n",
      "test accuracy =  0.8477423638778221\n",
      "\n",
      "case 3 : \n",
      "w = \n",
      " [ 0.25851661  0.3533387   2.33562776  0.7825921   0.33439916  0.07940036\n",
      " -0.08347987  0.23309134 -0.59278098 -0.9224849   0.11139573  1.25425869\n",
      " -0.38299462  0.41291781  0.04136013 -0.26411461  0.19283129 -0.42890321\n",
      "  0.42890321  0.23635122  0.30021361  1.03810521 -0.75216086 -0.45341371\n",
      " -0.02691157 -0.58252689 -2.00075384  0.75127891  0.82696618  0.52830705\n",
      "  0.89488995  0.14510375  0.18253094 -0.02584     0.00991404  0.89862005\n",
      "  0.68517003  0.23294385  0.24519932 -0.38363083 -0.08029608 -0.06493444\n",
      "  0.0453608   0.03743376 -0.01295908 -2.09374321  0.25763304  0.06659781\n",
      "  1.18748313  0.55059265 -0.47576614 -1.45842154  0.5822242  -1.06278329\n",
      " -0.00957211 -0.31704574  0.52485136  0.73044516  0.67457228 -0.6362418\n",
      " -0.00967268  0.17339114 -0.2364757   0.0375474   0.10120874 -0.24679341\n",
      "  0.23800627  0.64228457  1.00567032  0.23258942  0.42267607 -0.35336167\n",
      " -0.29178766 -0.38125401 -0.96291963 -0.45007954  0.512985    0.22019382\n",
      "  0.22640627 -0.04989104 -0.01836864 -0.95953334  0.01656805 -0.32741555\n",
      " -0.14011404  0.42856024 -0.84476927  0.75121645  0.76670733  0.07638782\n",
      "  0.26824615  0.44314098 -0.22058151 -0.04631789 -1.28758289 -0.57187068\n",
      "  1.82502288  1.39622511 -0.54691696 -1.05894077 -0.65551468 -0.38800489\n",
      " -3.36269032]\n",
      "test accuracy =  0.847675962815405\n"
     ]
    }
   ],
   "source": [
    "def accuracy(ypred, y_test):\n",
    "    acc = 1 - np.mean(np.abs(ypred - y_test))\n",
    "    return acc\n",
    "        \n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# case 1: lambda = 1 for all coefficients\n",
    "lambda_vec = np.identity(adult50kp['x_train'].shape[1]+1)\n",
    "\n",
    "print(\"case 1 : \")\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(adult50kp['x_train'], adult50kp['y_train'], verbal=True)\n",
    "ypred = logic1.predict(adult50kp['x_test'])\n",
    "print(\"test accuracy = \", accuracy(ypred, adult50kp['y_test']))\n",
    "print()\n",
    "\n",
    "# case 2: lambda = 1 for all but the intercept, no regularization for intercept term\n",
    "lambda_vec = np.identity(adult50kp['x_train'].shape[1]+1)\n",
    "lambda_vec[-1][-1] = 0\n",
    "\n",
    "print(\"case 2 : \")\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(adult50kp['x_train'], adult50kp['y_train'], verbal=True)\n",
    "ypred = logic1.predict(adult50kp['x_test'])\n",
    "print(\"test accuracy = \", accuracy(ypred, adult50kp['y_test']))\n",
    "print()\n",
    "\n",
    "# case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term\n",
    "lambda_vec = np.identity(adult50kp['x_train'].shape[1]+1)\n",
    "for i in range(len(lambda_vec)):\n",
    "    if i > 5:\n",
    "        lambda_vec[i][i] = 0.5\n",
    "    if i == len(lambda_vec)-1:\n",
    "        lambda_vec[i][i] = 0\n",
    "\n",
    "print(\"case 3 : \")\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(adult50kp['x_train'], adult50kp['y_train'], verbal=True)\n",
    "ypred = logic1.predict(adult50kp['x_test'])\n",
    "print(\"test accuracy = \", accuracy(ypred, adult50kp['y_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.3 Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters\n",
    "以下為我實作的方式，先對資料做random shuffle並且按照比例分為subtraining和tuning，而我有固定random seed方便reproduce，接著我設了一組grid，範圍為[0.01, 100]，設定$a_1 = a_2$並找出一個最佳的grid，接著分別固定$a_1$和$a_2$來找出連續性和非連續性資料最佳的$ \\lambda$，最後我找出的結果為$a_1 = 5, a_2 = 5$，並將這個結果做訓練後預測得到的test accuracy為$84.86$%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid = 0.01, test accuracy = 0.8478621146834604\n",
      "grid = 0.025, test accuracy = 0.848193569771296\n",
      "grid = 0.05, test accuracy = 0.848193569771296\n",
      "grid = 0.075, test accuracy = 0.848193569771296\n",
      "grid = 0.1, test accuracy = 0.848193569771296\n",
      "grid = 0.25, test accuracy = 0.8475306595956248\n",
      "grid = 0.5, test accuracy = 0.8478621146834604\n",
      "grid = 0.75, test accuracy = 0.848193569771296\n",
      "grid = 1, test accuracy = 0.848193569771296\n",
      "grid = 2.5, test accuracy = 0.849850845210474\n",
      "grid = 5, test accuracy = 0.8501823002983095\n",
      "grid = 7.5, test accuracy = 0.8495193901226383\n",
      "grid = 10, test accuracy = 0.8491879350348028\n",
      "grid = 25, test accuracy = 0.8488564799469672\n",
      "grid = 50, test accuracy = 0.848193569771296\n",
      "grid = 75, test accuracy = 0.849850845210474\n",
      "grid = 100, test accuracy = 0.848193569771296\n"
     ]
    }
   ],
   "source": [
    "# random shuffle\n",
    "np.random.seed(10)\n",
    "randomize = np.arange(len(adult50kp['x_train']))\n",
    "np.random.shuffle(randomize)\n",
    "x_subtrain = adult50kp['x_train'][randomize][:int(len(adult50kp['x_train'])*0.9)]\n",
    "x_tune = adult50kp['x_train'][randomize][int(len(adult50kp['x_train'])*0.9):]\n",
    "y_subtrain = adult50kp['y_train'][randomize][:int(len(adult50kp['y_train'])*0.9)]\n",
    "y_tune = adult50kp['y_train'][randomize][int(len(adult50kp['y_train'])*0.9):]\n",
    "\n",
    "\n",
    "# set the grid\n",
    "grid = [0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10, 25, 50, 75, 100]\n",
    "\n",
    "for i in range(len(grid)):\n",
    "    lambda_vec = grid[i] * np.identity(x_subtrain.shape[1]+1)\n",
    "    lambda_vec[-1][-1] = 0\n",
    "    \n",
    "    logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 100, tol = 1e-5, add_intercept = True)\n",
    "    logic1.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic1.predict(x_tune)\n",
    "    print(\"grid = \"+str(grid[i])+\", test accuracy = \"+str(accuracy(ypred, y_tune)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = 5, a2 = 0.01, test accuracy = 0.8485250248591316\n",
      "a1 = 5, a2 = 0.025, test accuracy = 0.8485250248591316\n",
      "a1 = 5, a2 = 0.05, test accuracy = 0.8485250248591316\n",
      "a1 = 5, a2 = 0.075, test accuracy = 0.8485250248591316\n",
      "a1 = 5, a2 = 0.1, test accuracy = 0.8485250248591316\n",
      "a1 = 5, a2 = 0.25, test accuracy = 0.8478621146834604\n",
      "a1 = 5, a2 = 0.5, test accuracy = 0.8471992045077892\n",
      "a1 = 5, a2 = 0.75, test accuracy = 0.8478621146834604\n",
      "a1 = 5, a2 = 1, test accuracy = 0.8478621146834604\n",
      "a1 = 5, a2 = 2.5, test accuracy = 0.8491879350348028\n",
      "a1 = 5, a2 = 5, test accuracy = 0.8501823002983095\n",
      "a1 = 5, a2 = 7.5, test accuracy = 0.8501823002983095\n",
      "a1 = 5, a2 = 10, test accuracy = 0.8495193901226383\n",
      "a1 = 5, a2 = 25, test accuracy = 0.8495193901226383\n",
      "a1 = 5, a2 = 50, test accuracy = 0.8485250248591316\n",
      "a1 = 5, a2 = 75, test accuracy = 0.8501823002983095\n",
      "a1 = 5, a2 = 100, test accuracy = 0.848193569771296\n"
     ]
    }
   ],
   "source": [
    "# set a1* = 5\n",
    "grid_best = 5\n",
    "for i in range(len(grid)):\n",
    "    lambda_vec = grid[i] * np.identity(x_subtrain.shape[1]+1)\n",
    "    for j in range(6):\n",
    "        lambda_vec[j][j] = grid_best\n",
    "    lambda_vec[-1][-1] = 0\n",
    "    \n",
    "    logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 100, tol = 1e-5, add_intercept = True)\n",
    "    logic1.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic1.predict(x_tune)\n",
    "    print(\"a1 = \"+str(grid_best)+\", a2 = \"+str(grid[i])+\", test accuracy = \"+str(accuracy(ypred, y_tune)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = 0.01, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.025, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.05, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.075, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.1, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.25, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.5, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 0.75, a2 = 5, test accuracy = 0.849850845210474\n",
      "a1 = 1, a2 = 5, test accuracy = 0.8501823002983095\n",
      "a1 = 2.5, a2 = 5, test accuracy = 0.8501823002983095\n",
      "a1 = 5, a2 = 5, test accuracy = 0.8501823002983095\n",
      "a1 = 7.5, a2 = 5, test accuracy = 0.8495193901226383\n",
      "a1 = 10, a2 = 5, test accuracy = 0.8488564799469672\n",
      "a1 = 25, a2 = 5, test accuracy = 0.8488564799469672\n",
      "a1 = 50, a2 = 5, test accuracy = 0.8491879350348028\n",
      "a1 = 75, a2 = 5, test accuracy = 0.8491879350348028\n",
      "a1 = 100, a2 = 5, test accuracy = 0.848193569771296\n"
     ]
    }
   ],
   "source": [
    "# set a2* = 5\n",
    "grid_best = 5\n",
    "for i in range(len(grid)):\n",
    "    lambda_vec = grid_best * np.identity(x_subtrain.shape[1]+1)\n",
    "    for j in range(6):\n",
    "        lambda_vec[j][j] = grid[i]\n",
    "    lambda_vec[-1][-1] = 0\n",
    "    \n",
    "    logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 100, tol = 1e-5, add_intercept = True)\n",
    "    logic1.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic1.predict(x_tune)\n",
    "    print(\"a1 = \"+str(grid[i])+\", a2 = \"+str(grid_best)+\", test accuracy = \"+str(accuracy(ypred, y_tune)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy =  0.848605577689243\n"
     ]
    }
   ],
   "source": [
    "# a1* = 5, a2* = 5\n",
    "a1 = 5\n",
    "a2 = 5\n",
    "lambda_vec = a2 * np.identity(x_subtrain.shape[1]+1)\n",
    "for i in range(6):\n",
    "    lambda_vec[i][i] = a1\n",
    "lambda_vec[-1][-1] = 0\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "ypred = logic1.predict(adult50kp['x_test'])\n",
    "print(\"test accuracy = \", accuracy(ypred, adult50kp['y_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.4 Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning). Compare the estimated parameters and test accuracy with those from your own models\n",
    "以下為我的實作方式，先利用上一小題的grid set來對sklearn的logistic regression model做hyperparameter tuning，而我找出最佳的grid為0.25，而放進LogisticRegression的參數C，為regularization中$ \\lambda$的倒數，也就是$ \\frac{1}{ \\lambda}$，因此找到最佳的$ \\lambda$為4，跟我自己實作的model做hyperparameter tuning得出的結果相似，而sklearn的logistic regression model最後得到的test accuracy為$84.87$%，比我實作的model略好一些，而兩者使用的optimizer也都為newton method，也不對截距項做regularization，其中會有差別的地方我認為是初始化weight的方法不同，以我實作的model為例，利用ridge regression得到的weight做prediction的test accuracy只有約$44$%，但是sklearn的logistic regression model卻能夠有約$75$%的test accuarcy，雖然兩者model最後都訓練至收斂，但這初始化方法的不同可能就是最後結果有些許差異的主要原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid = 0.01, test accuracy = 0.8468677494199536\n",
      "grid = 0.025, test accuracy = 0.8485250248591316\n",
      "grid = 0.05, test accuracy = 0.8491879350348028\n",
      "grid = 0.075, test accuracy = 0.8491879350348028\n",
      "grid = 0.1, test accuracy = 0.8488564799469672\n",
      "grid = 0.25, test accuracy = 0.8508452104739808\n",
      "grid = 0.5, test accuracy = 0.8495193901226383\n",
      "grid = 0.75, test accuracy = 0.848193569771296\n",
      "grid = 1, test accuracy = 0.848193569771296\n",
      "grid = 2.5, test accuracy = 0.8478621146834604\n",
      "grid = 5, test accuracy = 0.8478621146834604\n",
      "grid = 7.5, test accuracy = 0.848193569771296\n",
      "grid = 10, test accuracy = 0.848193569771296\n",
      "grid = 25, test accuracy = 0.848193569771296\n",
      "grid = 50, test accuracy = 0.8478621146834604\n",
      "grid = 75, test accuracy = 0.8478621146834604\n",
      "grid = 100, test accuracy = 0.8478621146834604\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning of sklearn model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "for i in range(len(grid)):\n",
    "    lr2 = LogisticRegression(solver = 'newton-cg', C = grid[i], max_iter = 1000, fit_intercept = False)\n",
    "    lr2.fit(x_subtrain, y_subtrain)\n",
    "    ypred = lr2.predict(x_tune)\n",
    "    print(\"grid = \"+str(grid[i])+\", test accuracy = \"+str(accuracy(ypred, y_tune)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.25, 'class_weight': None, 'dual': False, 'fit_intercept': False, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "test accuracy =  0.8487383798140771\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression(solver = 'newton-cg', C = 0.25, max_iter = 1000, fit_intercept = False)\n",
    "lr2.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "ypred = lr2.predict(adult50kp['x_test'])\n",
    "print(lr2.get_params())\n",
    "print(\"test accuracy = \", accuracy(ypred, adult50kp['y_test']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
