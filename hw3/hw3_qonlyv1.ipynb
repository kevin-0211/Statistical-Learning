{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習\n",
    "### Homework 3\n",
    "\n",
    "\n",
    "請將IPYNB檔上傳至COOL作業區。回答作業時建議使用 \"三明治\" 答題法。也就是說，先說明要做什麼，然後列出程式碼與結果，最後說明這些結果的意義。作業自己做。嚴禁抄襲。不接受紙本繳交，不接受遲交。請以英文或中文作答。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一題 [分類器、特徵建構、與Stacking]\n",
    "\n",
    "(50%) 建構分類器時常會透過特徵選擇與Ensemble增強預測能力。本題的目的是讓大家練習這些技巧。本題使用一個中文姓名分類的資料集。這個資料集收集了10730個中文名(不含姓)，以及這些名子對應到的性別。本資料集的資料檔是**namesex_data_v2.csv**，共有三個欄位，gname, sex, fold:\n",
    "* gname: 不含姓的中文名。\n",
    "* sex: 1為男性，0為女性。\n",
    "* fold: 為0 - 9的整數，做為後續切割資料集使用。\n",
    "\n",
    "我們先將這個資料集讀入Numpy Array，並列印前十筆資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "f = open('ds/namesex_data_v2.csv', 'r', encoding='utf8')\n",
    "mydata = csv.DictReader(f)\n",
    "sexlist = []\n",
    "namelist = []\n",
    "foldlist = []\n",
    "for i, arow in enumerate(mydata):\n",
    "    if i < 10:\n",
    "        print(arow)\n",
    "    sexlist.append(int(arow['sex'].strip()))\n",
    "    gname = arow['gname'].strip()\n",
    "    namelist.append(gname)\n",
    "    foldlist.append(int(arow['fold'].strip()))\n",
    "\n",
    "sexlist = np.asarray(sexlist)\n",
    "namelist = np.asarray(namelist)\n",
    "foldlist = np.asarray(foldlist)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來看一些統計數字:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = namelist.shape[0]\n",
    "print(\"資料筆數 = \", nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sex = np.mean(sexlist)\n",
    "print(\"男性比率\", avg_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個資料集的男女比率還算是平均，男性佔了47.94%。\n",
    "\n",
    "我們用了比較迂迴的方式讀檔案，Pandas其實可以直接讀csv檔。接下來做一些基本的資料分析。我們關心幾件事:\n",
    "* 最常見的名子\n",
    "* 共有多少個不重複的名子\n",
    "* 有多少難判斷男女的名子\n",
    "\n",
    "要回答這些問題，首先對名子加總，計算數量與男性比率，排序之後列出前20個名子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "name_df = pd.DataFrame({'sex': sexlist, 'name': namelist, 'fold': foldlist})\n",
    "sex_tend = name_df[['name', 'sex']].groupby([\"name\"]).agg(['mean', 'count']).reset_index()\n",
    "sex_tend.columns = ['-'.join(col).strip() for col in sex_tend.columns.values]\n",
    "sex_tend = sex_tend.sort_values(['sex-count'], ascending=False)\n",
    "\n",
    "print(\"最常見的20個名子:\")\n",
    "sex_tend.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的列表看來，最常見的男性名子是承恩與冠廷，最常見的女性名子是宜蓁與佳穎。而這些常見的名子男性比率都非常接近0或1，表示這些名子沒有性別混淆的問題。\n",
    "\n",
    "另外一個問題是有多少名子只出現一次:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_name = np.sum(sex_tend['sex-count'] <= 1)\n",
    "print(\"出現一次名子個數: \", single_name, \"比率:\", single_name / nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由結果看來，名子在資料集中只出現一次的比率很高，有65%。也就是說，如果單純的使用訓練資料中出現的名子做為特徵，模型應該會有很差的預測能力。\n",
    "\n",
    "#### <font color=#800000>為了處理這個問題，我們在接下來的任務中，會使用\"Unigram + Full Name\"的特徵表示，比如說，\"芳瑜\"的特徵表示為\"芳\"、\"瑜\"，以及原始的名子\"芳瑜\"。</font>\n",
    "\n",
    "最後我們來看看性別混淆的名子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = (sex_tend['sex-mean'] > 0.4) & (sex_tend['sex-mean'] < 0.6)\n",
    "sex_amb = sex_tend[ind1]\n",
    "amb_count = sex_amb['sex-count'].sum()\n",
    "print(\"難區分性別姓名數量: \", amb_count, \"比率:\", amb_count/ nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "難以區分性別的名子總數並不高，所占總體資料的比率也很低，只有1.26%，因此不用擔心這個因素會影響預測準確率。\n",
    "\n",
    "下面來看看幾個容易混淆性別的名子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_amb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 實做限制\n",
    "為了確保可以確實練習到重要技巧，禁用Pycaret (<https://pycaret.org/>) 這類可以自動化調教參數與Ensemble模型的工具。另外也禁止使用`sklearn.ensemble.Stacking.*`。你可以使用sklearn中Pre-processing工具函數與現成的分類器，但參數調教請自行處理。\n",
    "\n",
    "\n",
    "\n",
    "#### 回答下面問題\n",
    "#### Q1.1 (10%): \n",
    "使用One-hot Encoding建構資料變數。所有資料變數都要是Numpy Array。依照每筆資料其fold值切割為Training (fold <=6)、Validation (fold == 7)、Stacking (fold == 8)、Test (fold == 9)。每個資料集應有特徵Array(二維)以及Label Array(一維)。如前面提到的，每個名子應該要對應到全名以及單字的One-hot Encoding。比如說，\"芳瑜\"的特徵表示為\"芳\"、\"瑜\"，以及原始的名子\"芳瑜\"。建構特徵表示時應依照個特徵出現的頻率篩選。特徵在訓練資料出現兩次或以上才納入。如果一個特徵被排除，這個特徵出現時應被歸為\"\\_Other_Feature_\"。任何名子只要有出現未被納入的特徵，則其\"\\_Other_Feature_\"的欄位值為1。\n",
    "\n",
    "舉例而言，假設訓練資料集中有下面三個名子: 承恩、承德、恩賜。在經過特徵頻率篩選之後，只剩下以下特徵: 承、恩。其他特徵，如承恩、承德、恩賜、德、賜皆被排除。因此最後的特徵表示為:\n",
    "\n",
    "Input | 承 | 恩 | \\_Other_Feature_ |\n",
    "------|----|----|-----------------|\n",
    "承恩  | 1  | 1   |  1    |\n",
    "承德  | 1  | 0   |  1  |\n",
    "恩賜  | 0  | 1   |  1 |\n",
    "\n",
    "最後應產生以下Numpy Array:\n",
    "* x_train: Training Features\n",
    "* y_train: Training Labels\n",
    "* x_valid: Validation Features\n",
    "* y_valid: Validation Labels\n",
    "* x_stack: Stack Features\n",
    "* y_stack: Stack Labels\n",
    "* x_test: Test Features\n",
    "* y_test: Test Labels\n",
    "\n",
    "請列出每個Numpy Array的Shape以資查驗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.2 (10%):\n",
    "使用`sklearn.linear_model.LogisticRegression()`建構Logistic Regression分類模型。利用Training與Validation調教Regularization Coefficient $c$。Grid Search的小值為$10^{-4}$，最大值是$1,000$，總共取20個點，使用等比級數間隔。取F-1 Score最大之Regularization Coefficient，印出此數值(以下稱bestc)。將Training與Validation合併後，令Regularizaiton Coefficient為bestc，訓練最終模型，並報告Test Data的Accuracy, Precision, Recall, 與F-1 Score。另外列出係數絕對值最大的20個特徵。注意應列出未取絕對值的係數數值，方知特徵的性別傾向。討論Prediction Performance與重要特徵的合理性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.3 (10%):\n",
    "使用`sklearn.ensemble.RandomForestClassifier()`建構Random Forest分類模型。利用Training與Validation調教Number of Tress (i.e., n_estimators)。Grid Search的小值為$5$，最大值是$1,000$，總共取10個點，使用等比級數間隔。取F-1 Score最大之n_estimators，印出此數值(best_nest)。將Training與Validation合併後，令n_estimators為best_nest，訓練最終模型，並報告Test Data的Accuracy, Precision, Recall, 與F-1 Score。另外列出最重要的20個特徵。討論Prediction Performance與重要特徵的合理性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.4 (10%):\n",
    "使用`sklearn.ensemble.GradientBoostingClassifier()`建構Gradient Boosting Decision Tree (GBDT)分類模型。利用Training與Validation調教learning_rate與n_estimators。考慮以下Learning Rate: 0.1, 0.5, 1。每一個Learning Rate設n_estimator為1,500並估計一個GBDT分類器，計算1,500個Stages下Validation F-1 Score值，繪圖，並找出讓F-1 Score最大的Stage與F-1 Score最大值。對所有Learning Rate重複同樣程序，找出最佳的Learning Rate (稱best_lr) 與其對應的Number of Stages(best_nstg)。將Training與Validation合併後，令learning_rate為best_lr, n_estimators為best_nstg，訓練最終模型，並報告Test Data的Accuracy, Precision, Recall, 與F-1 Score。另外列出最重要的20個特徵。討論Prediction Performance與重要特徵的合理性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.5 (10%):\n",
    "取用前面所建構的Logistic Regression, Random Forest, 與Gradient Boosting Decision Tree, 組合(Stacking)成一個新的分類器。我們使用Logistic Regression without Penalty建構這個Stacking分類器。訓練資料為Stacking資料集。各分類器輸入的特徵為男生預測機率(注意: 不是分類結果)。列出Stacking分類器的係數，討論係數的意義，並計算這個Stacking分類器在Test資料集的預測能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Solution\n",
    "------\n",
    "請自行增加Cell並回答問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二題 [Data Visualization via Dimensionality Reduction]\n",
    "\n",
    "Data visualization is a useful technique that can help us understand the characteristics of a dataset. We are going to practice this skill using the  University Offer of Admission Dataset (UDOAD).  \n",
    "\n",
    "#### Dataset: University Department Offer of Admission Dataset (UDOAD; 大學推薦甄選資料集)\n",
    "\n",
    "Many high school students get admitted to universities through an application and screening process that requires each university department to offer admission to applicants first before students can choose where they want to go. Suppose we think of applicants as the customers of an academic department. In that case, the duplications of offered applicants from different departments can be used to understand the competitive relationships between academic departments. We are going to visualize this competition relationship using UDOAD. \n",
    "\n",
    "UDOAD was collected through a popular online offer searching service (https://freshman.tw/; site no longer operational) for the 2017 academic year. We collected the offers received by each applicant as well as the basic information for academic departments. This dataset contains two files\n",
    "* student_admission106.csv: the offers received by each student applicant. \n",
    "* univ_name106short1.csv: the basic information, such as the name and field of academic departments. \n",
    "\n",
    "Below is the first few records of these two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "stu_adm = pd.read_csv('ds/student_admission106.csv', encoding=\"utf-8\", dtype=str)\n",
    "uname = pd.read_csv('ds/univ_name106short1.csv', encoding=\"utf-8\", dtype=str)\n",
    "\n",
    "all_depid = stu_adm['department_id'].unique()\n",
    "all_stuid = stu_adm['student_id'].unique()\n",
    "\n",
    "ndepid = all_depid.shape[0]\n",
    "nstuid = all_stuid.shape[0]\n",
    "print(\"There are %d students and %d departments in total.\" % (nstuid, ndepid))\n",
    "\n",
    "print(\"offers received by students:\")\n",
    "stu_adm.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The department_id can uniquely identify an academic department. We do not care about the ranking of admission here, and you should ignore the \"state\" column. We only care about the \"co-application\" relations in this dataset. You should use student_id to identify a student applicant uniquely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"academic department basic information:\")\n",
    "uname.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this dataset to identify the name of a department_id. The school_name and department_name contain the \"full name\" of an academic department. To facilitate visualization, we also provide \"shorter names\" in school_name_abbr and department_name_abbr. The category_name is the field of an academic department. This field is essential in our visualization exercise since you should color each data point according to its category_name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Restrictions\n",
    "You are allowed to use any Python libraries in your solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Our focus is on the relationships between departments. To do this, we need to convert the raw data into a \"matrix\" representation. Each row represents an academic department, and each column represents a student applicant. The cell's value is one if a student applied for admission to the corresponding academic department, and 0 otherwise. \n",
    "\n",
    "To avoid potential numerical problems, we only include an academic department if it received ten or more applications. Moreover, we only include a student applicant if he or she applied for more than one academic department. \n",
    "\n",
    "Note that the two conditions should be satisfied \"as is\" in after preprocessing. For example, suppose a student applied for two departments in the original dataset, and one of the departments was removed. In that case, this student should be removed because the student only applied for one department in the processed dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "stu_adm = pd.read_csv('ds/student_admission106.csv', encoding=\"utf-8\", dtype=str)\n",
    "uname = pd.read_csv('ds/univ_name106short1.csv', encoding=\"utf-8\", dtype=str)\n",
    "\n",
    "all_depid = stu_adm['department_id'].unique()\n",
    "all_stuid = stu_adm['student_id'].unique()\n",
    "\n",
    "ndepid = all_depid.shape[0]\n",
    "nstuid = all_stuid.shape[0]\n",
    "print(\"In raw data, there are %d students and %d departments in total.\" % (nstuid, ndepid))\n",
    "\n",
    "#construct the department-student matrix (i.e. array). \n",
    "dep_stu = np.zeros((ndepid, nstuid))\n",
    "rowname = all_depid.copy()\n",
    "\n",
    "depid_seq_map = dict()\n",
    "for i in range(ndepid):\n",
    "    depid_seq_map[all_depid[i]] = i\n",
    "\n",
    "stuid_seq_map = dict()\n",
    "for i in range(nstuid):\n",
    "    stuid_seq_map[all_stuid[i]] = i\n",
    "\n",
    "for cindex, row in stu_adm.iterrows():\n",
    "    #print(cindex, row)\n",
    "    dep_seq = depid_seq_map[row['department_id']]\n",
    "    stu_seq = stuid_seq_map[row['student_id']]\n",
    "    #print(dep_seq, stu_seq)\n",
    "    dep_stu[dep_seq, stu_seq] = 1\n",
    "\n",
    "#Remove very small departments.\n",
    "min_stu_per_dep = 10\n",
    "min_apply_dep_per_stu = 2\n",
    "\n",
    "#remove small departments and single-application students. \n",
    "dep_apply_sum = np.sum(dep_stu, axis = 1)\n",
    "keeprow = dep_apply_sum >= min_stu_per_dep\n",
    "rowname = rowname[keeprow]\n",
    "dep_stu2 = dep_stu[keeprow,:]\n",
    "stu_apply_sum = np.sum(dep_stu2, axis = 0)\n",
    "dep_stu2 = dep_stu2[:, stu_apply_sum >= min_apply_dep_per_stu]\n",
    "\n",
    "#another run of filtering\n",
    "dep_apply_sum = np.sum(dep_stu2, axis = 1)\n",
    "dep_stu2 = dep_stu2[dep_apply_sum >= min_stu_per_dep,:]\n",
    "rowname = rowname[dep_apply_sum >= min_stu_per_dep]\n",
    "stu_apply_sum = np.sum(dep_stu2, axis = 0)\n",
    "dep_stu2 = dep_stu2[:, stu_apply_sum >= min_apply_dep_per_stu]\n",
    "\n",
    "#third run of filtering\n",
    "dep_apply_sum = np.sum(dep_stu2, axis = 1)\n",
    "dep_stu2 = dep_stu2[dep_apply_sum >= min_stu_per_dep,:]\n",
    "rowname = rowname[dep_apply_sum >= min_stu_per_dep]\n",
    "\n",
    "stu_apply_sum = np.sum(dep_stu2, axis = 0)\n",
    "dep_stu2 = dep_stu2[:, stu_apply_sum >= min_apply_dep_per_stu]\n",
    "\n",
    "#check to make sure the two conditions are satisfied. \n",
    "dep_apply_sum = np.sum(dep_stu2, axis = 1)\n",
    "print(\"Number of department too small:\", np.sum(dep_apply_sum < min_stu_per_dep))\n",
    "stu_apply_sum = np.sum(dep_stu2, axis = 0)\n",
    "print(\"Number of students applying only one department:\", np.sum(stu_apply_sum <min_apply_dep_per_stu))\n",
    "\n",
    "#now both conditions are satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the number of academic departments and student applicants in the processed dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dep, num_stu = dep_stu2.shape\n",
    "print(\"In final dataset, there are\", num_dep, \"academic departments and\", num_stu, \"student applicants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the top ten departments that received the most applications and the number of applications they received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uname['depname'] = uname.school_name_abbr + uname.department_name_abbr\n",
    "uname2 = uname[['department_id', 'depname', 'category_name']].copy()\n",
    "#this is for later use, to color data points. \n",
    "category_id, category_levels = pd.factorize(uname2.category_name)\n",
    "#uname2['category_id'] = category_id / np.max(category_id)\n",
    "uname2['category_id'] = category_id\n",
    "\n",
    "#create a data frame for column name\n",
    "colname_df = pd.DataFrame({'department_id': rowname})\n",
    "colname_df = colname_df.merge(uname2, how = \"left\", on=\"department_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topdepid = np.argsort(dep_apply_sum)[::-1]\n",
    "topn = 10\n",
    "topdep = pd.DataFrame({'department_id': rowname[topdepid[0:topn]], \n",
    "                       'department_name': colname_df.depname.values[topdepid[0:topn]],\n",
    "                       'num_applicant': dep_apply_sum[topdepid[0:topn]]\n",
    "                       }) \n",
    "topdep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following questions\n",
    "Visualize academic departments in the following questions. In all plots, you should color data points according to the academic department's category. Moreover, you should provide a legend or a picture that illustrates the mapping between colors and category names. Visualize the data using two-dimensional plots. Note that it is your responsibility to study the documentation of your choice libraries and make sure that the results are reasonable. \n",
    "* Q2.1 (10%): Visualize academic departments using the first eight principal components. Use your judgment to select multiple pairs of principal components to visualize. Discuss the visual patterns concerning department categories. \n",
    "* Q2.2 (10%): Visualize academic departments using multiple dimensional scaling. Consider both the metric and non-metric settings. Discuss the result. \n",
    "* Q2.3 (10%): Visualize academic departments using Locally Linear Embedding. Consider three variations: (1) Use 20 neighbors to construct the weight matrix; (2) Use 40 neighbors to construct the weight matrix; (3) Perform PCA transformation first, and use the first 100 principal components as the input to LLE (with 20 neighbors). Discuss the result. \n",
    "* Q2.4 (10%): Visualize academic departments using Kernel PCA. You should at least consider the RBF and Cosine kernel. It is your responsibility to select reasonably good kernel parameters. Discuss the result. \n",
    "* Q2.5 (10%): Visualize academic departments using t-SNE. You should consider at least the Euclidian, Cosine, and Jaccard metric. Set numpy random seed so that your results can be repeated. Discuss the result. \n",
    "* Q2.6 (10%) Select the most promising visualization method in the previous question and refine the result. You should color points by department category. Label each data point with its name so that we can quickly identify a data point on the picture. Moreover, you should try to reduce the problem caused by overlapping points and labels. Output an image that is large enough so that a user can easily identify a department and its neighbors. Jupyter Lab has limitations on the largest picture size. To overcome this problem, output the picture to a separate file and submit the file for grading. Your score depends on how useful, readable, and visually pleasing of your visualization results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution\n",
    "---\n",
    "\n",
    "Add cells below and answer the questions below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
