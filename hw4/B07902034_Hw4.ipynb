{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (463715, 90)\n",
      "X_subtrain shape =  (417344, 90)\n",
      "X_valid shape =  (46371, 90)\n",
      "Y_subtrain shape =  (417344,)\n",
      "Y_valid shape =  (46371,)\n",
      "X_test shape =  (51630, 90)\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Load data\n",
    "with open('msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test'].astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_mean = Y_train.mean()\n",
    "Y_train_keep = Y_train.copy()\n",
    "Y_test_keep = Y_test.copy()\n",
    "Y_train = Y_train - y_mean\n",
    "Y_test = Y_test - y_mean\n",
    "\n",
    "\n",
    "#validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print(\"X_test shape = \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(Y_train, sm.add_constant(X_train))\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.30975265 -2.88088114 -1.53234348  0.05737583 -0.33952889]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(results.params[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms =  9.510160684544399\n"
     ]
    }
   ],
   "source": [
    "ypred = results.predict(sm.add_constant(X_test))\n",
    "rms = np.sqrt(mean_squared_error(ypred, Y_test))\n",
    "print(\"rms = \", rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(90, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        return torch.sum(torch.pow((x - y), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(H, device):\n",
    "    model = MLP(H).to(device)\n",
    "    loss = SSE()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr= 0.00001)\n",
    "    num_epoch = 100\n",
    "\n",
    "    flag = 0\n",
    "    batch_cnt = 0\n",
    "    best_step_cnt = 0\n",
    "    best_rmse = math.inf\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = torch.reshape(model(data[0]), (len(data[0]), ))\n",
    "            batch_loss = loss(train_pred, data[1])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            batch_cnt += 1\n",
    "            best_step_cnt += 1\n",
    "            if batch_cnt % 100 == 0:\n",
    "                train_pred = model(torch.FloatTensor(X_subtrain))\n",
    "                train_rmse = np.sqrt(mean_squared_error(train_pred.detach().numpy(), Y_subtrain))\n",
    "                valid_pred = model(torch.FloatTensor(X_valid))\n",
    "                valid_rmse = np.sqrt(mean_squared_error(valid_pred.detach().numpy(), Y_valid))\n",
    "                if valid_rmse < best_rmse:\n",
    "                    print('Train: %f, Valid: %f' % (train_rmse, valid_rmse))\n",
    "                    best_rmse = valid_rmse\n",
    "                    best_step_cnt = 0\n",
    "                    torch.save(model, 'best_H'+str(H)+'.pkl')\n",
    "            if best_step_cnt >= 5000:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Loss: %3.6f' % \\\n",
    "          (epoch + 1, num_epoch, time.time()-epoch_start_time, train_loss/train_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 1000\n",
    "train_set = dataset(X_subtrain, Y_subtrain)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9.449331, Valid: 9.379689\n",
      "Train: 9.105864, Valid: 9.029032\n",
      "Train: 8.955000, Valid: 8.894309\n",
      "Train: 8.880700, Valid: 8.834023\n",
      "[001/100] 5.97 sec(s) Loss: 88.909528\n",
      "Train: 8.864705, Valid: 8.833821\n",
      "Train: 8.787385, Valid: 8.753699\n",
      "Train: 8.715527, Valid: 8.694748\n",
      "[002/100] 5.40 sec(s) Loss: 78.068211\n",
      "Train: 8.701793, Valid: 8.686587\n",
      "Train: 8.691593, Valid: 8.683795\n",
      "Train: 8.682180, Valid: 8.679771\n",
      "Train: 8.660342, Valid: 8.657649\n",
      "[003/100] 5.68 sec(s) Loss: 76.482771\n",
      "Train: 8.654021, Valid: 8.652126\n",
      "Train: 8.611747, Valid: 8.625824\n",
      "[004/100] 5.59 sec(s) Loss: 75.413951\n",
      "Train: 8.592249, Valid: 8.619514\n",
      "Train: 8.591910, Valid: 8.612529\n",
      "[005/100] 5.50 sec(s) Loss: 74.702658\n",
      "Train: 8.571875, Valid: 8.608444\n",
      "[006/100] 6.04 sec(s) Loss: 74.236598\n",
      "Train: 8.543302, Valid: 8.604204\n",
      "Train: 8.526733, Valid: 8.602380\n",
      "[007/100] 6.01 sec(s) Loss: 73.664182\n",
      "Train: 8.512384, Valid: 8.587558\n",
      "[008/100] 5.84 sec(s) Loss: 73.365816\n",
      "[009/100] 5.57 sec(s) Loss: 73.037205\n",
      "Train: 8.465723, Valid: 8.572212\n",
      "[010/100] 5.87 sec(s) Loss: 72.753125\n",
      "Train: 8.443000, Valid: 8.557301\n",
      "[011/100] 5.77 sec(s) Loss: 72.495240\n",
      "[012/100] 5.83 sec(s) Loss: 72.250892\n",
      "Train: 8.422751, Valid: 8.555868\n",
      "[013/100] 5.39 sec(s) Loss: 72.072967\n",
      "[014/100] 5.40 sec(s) Loss: 71.851834\n",
      "[015/100] 5.37 sec(s) Loss: 71.625775\n",
      "[016/100] 5.38 sec(s) Loss: 71.484136\n",
      "[017/100] 5.55 sec(s) Loss: 71.318138\n",
      "[018/100] 5.48 sec(s) Loss: 71.113036\n",
      "[019/100] 5.37 sec(s) Loss: 70.968275\n",
      "[020/100] 5.40 sec(s) Loss: 70.850764\n",
      "[021/100] 5.40 sec(s) Loss: 70.706290\n",
      "[022/100] 5.37 sec(s) Loss: 70.557598\n",
      "[023/100] 5.62 sec(s) Loss: 70.387862\n",
      "[024/100] 5.37 sec(s) Loss: 70.235461\n"
     ]
    }
   ],
   "source": [
    "# H=45\n",
    "training(45, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.8407135\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_H45.pkl')\n",
    "model.eval()\n",
    "test_pred = model(torch.FloatTensor(X_test))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9.187493, Valid: 9.140050\n",
      "Train: 8.953436, Valid: 8.899212\n",
      "Train: 8.864867, Valid: 8.812171\n",
      "Train: 8.798080, Valid: 8.765556\n",
      "[001/100] 8.30 sec(s) Loss: 86.777884\n",
      "Train: 8.786918, Valid: 8.758759\n",
      "Train: 8.733521, Valid: 8.703835\n",
      "[002/100] 7.57 sec(s) Loss: 77.310654\n",
      "Train: 8.644323, Valid: 8.633585\n",
      "Train: 8.616182, Valid: 8.617191\n",
      "[003/100] 7.59 sec(s) Loss: 75.560333\n",
      "Train: 8.579114, Valid: 8.608191\n",
      "Train: 8.539529, Valid: 8.586229\n",
      "[004/100] 7.54 sec(s) Loss: 74.445112\n",
      "Train: 8.507303, Valid: 8.558593\n",
      "[005/100] 7.25 sec(s) Loss: 73.441095\n",
      "Train: 8.459901, Valid: 8.555553\n",
      "[006/100] 7.74 sec(s) Loss: 72.723072\n",
      "Train: 8.426862, Valid: 8.552495\n",
      "Train: 8.378786, Valid: 8.537673\n",
      "[007/100] 7.30 sec(s) Loss: 72.153218\n",
      "Train: 8.371078, Valid: 8.527810\n",
      "[008/100] 7.18 sec(s) Loss: 71.471137\n",
      "[009/100] 7.24 sec(s) Loss: 70.855340\n",
      "[010/100] 7.18 sec(s) Loss: 70.377707\n",
      "[011/100] 7.24 sec(s) Loss: 69.857937\n",
      "Train: 8.247368, Valid: 8.522726\n",
      "[012/100] 7.77 sec(s) Loss: 69.491703\n",
      "[013/100] 7.19 sec(s) Loss: 68.923086\n",
      "[014/100] 7.26 sec(s) Loss: 68.518150\n",
      "[015/100] 7.27 sec(s) Loss: 68.038312\n",
      "[016/100] 7.47 sec(s) Loss: 67.670697\n",
      "[017/100] 7.86 sec(s) Loss: 67.357595\n",
      "[018/100] 7.21 sec(s) Loss: 66.992399\n",
      "[019/100] 7.19 sec(s) Loss: 66.447040\n",
      "[020/100] 7.30 sec(s) Loss: 66.158709\n",
      "[021/100] 7.27 sec(s) Loss: 65.845192\n",
      "[022/100] 7.27 sec(s) Loss: 65.523939\n"
     ]
    }
   ],
   "source": [
    "# H=90\n",
    "training(90, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.825038\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_H90.pkl')\n",
    "model.eval()\n",
    "test_pred = model(torch.FloatTensor(X_test))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9.254824, Valid: 9.218730\n",
      "Train: 8.959325, Valid: 8.910476\n",
      "Train: 8.787598, Valid: 8.756402\n",
      "[001/100] 13.13 sec(s) Loss: 86.119623\n",
      "Train: 8.706698, Valid: 8.684003\n",
      "Train: 8.660354, Valid: 8.654922\n",
      "[002/100] 11.91 sec(s) Loss: 76.881483\n",
      "Train: 8.618110, Valid: 8.629775\n",
      "Train: 8.576658, Valid: 8.601912\n",
      "Train: 8.545632, Valid: 8.589115\n",
      "[003/100] 11.90 sec(s) Loss: 74.889701\n",
      "Train: 8.515756, Valid: 8.584311\n",
      "Train: 8.481317, Valid: 8.570221\n",
      "[004/100] 12.04 sec(s) Loss: 73.592101\n",
      "Train: 8.446346, Valid: 8.546415\n",
      "Train: 8.398228, Valid: 8.531609\n",
      "[005/100] 11.97 sec(s) Loss: 72.416991\n",
      "Train: 8.342570, Valid: 8.515935\n",
      "Train: 8.340707, Valid: 8.510673\n",
      "Train: 8.308944, Valid: 8.509221\n",
      "[006/100] 13.34 sec(s) Loss: 71.493427\n",
      "Train: 8.286865, Valid: 8.504952\n",
      "Train: 8.258041, Valid: 8.489493\n",
      "[007/100] 11.97 sec(s) Loss: 70.606164\n",
      "[008/100] 11.94 sec(s) Loss: 69.649325\n",
      "[009/100] 11.95 sec(s) Loss: 68.691412\n",
      "[010/100] 11.96 sec(s) Loss: 67.863106\n",
      "[011/100] 12.11 sec(s) Loss: 66.913194\n",
      "[012/100] 13.21 sec(s) Loss: 65.823769\n",
      "[013/100] 11.95 sec(s) Loss: 65.127926\n",
      "[014/100] 12.01 sec(s) Loss: 64.178770\n",
      "[015/100] 11.96 sec(s) Loss: 63.402951\n",
      "[016/100] 12.00 sec(s) Loss: 62.480663\n",
      "[017/100] 13.37 sec(s) Loss: 61.436648\n",
      "[018/100] 11.99 sec(s) Loss: 60.637250\n"
     ]
    }
   ],
   "source": [
    "# H=180\n",
    "training(180, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.86847\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_H180.pkl')\n",
    "model.eval()\n",
    "test_pred = model(torch.FloatTensor(X_test))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_q4(H, w, device):\n",
    "    model = MLP(H).to(device)\n",
    "    loss = SSE()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr= 0.00001, weight_decay=w)\n",
    "    num_epoch = 100\n",
    "\n",
    "    flag = 0\n",
    "    batch_cnt = 0\n",
    "    best_step_cnt = 0\n",
    "    best_rmse = math.inf\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = torch.reshape(model(data[0]), (len(data[0]), ))\n",
    "            batch_loss = loss(train_pred, data[1])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            batch_cnt += 1\n",
    "            best_step_cnt += 1\n",
    "            if batch_cnt % 100 == 0:\n",
    "                train_pred = model(torch.FloatTensor(X_subtrain))\n",
    "                train_rmse = np.sqrt(mean_squared_error(train_pred.detach().numpy(), Y_subtrain))\n",
    "                valid_pred = model(torch.FloatTensor(X_valid))\n",
    "                valid_rmse = np.sqrt(mean_squared_error(valid_pred.detach().numpy(), Y_valid))\n",
    "                if valid_rmse < best_rmse:\n",
    "                    print('Train: %f, Valid: %f' % (train_rmse, valid_rmse))\n",
    "                    best_rmse = valid_rmse\n",
    "                    best_step_cnt = 0\n",
    "                    torch.save(model, 'best_H'+str(H)+'_w'+str(w)+'.pkl')\n",
    "            if best_step_cnt >= 5000:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Loss: %3.6f' % \\\n",
    "          (epoch + 1, num_epoch, time.time()-epoch_start_time, train_loss/train_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9.292967, Valid: 9.232933\n",
      "Train: 8.974434, Valid: 8.910072\n",
      "Train: 8.927661, Valid: 8.873037\n",
      "Train: 8.859359, Valid: 8.813032\n",
      "[001/100] 5.36 sec(s) Loss: 88.143067\n",
      "Train: 8.784674, Valid: 8.748822\n",
      "Train: 8.749503, Valid: 8.726721\n",
      "Train: 8.695510, Valid: 8.677515\n",
      "[002/100] 5.33 sec(s) Loss: 77.840285\n",
      "Train: 8.653134, Valid: 8.646994\n",
      "[003/100] 5.39 sec(s) Loss: 76.156295\n",
      "Train: 8.635962, Valid: 8.639280\n",
      "Train: 8.628282, Valid: 8.630695\n",
      "Train: 8.601198, Valid: 8.608505\n",
      "[004/100] 5.36 sec(s) Loss: 75.232691\n",
      "Train: 8.586085, Valid: 8.601299\n",
      "[005/100] 5.35 sec(s) Loss: 74.630244\n",
      "Train: 8.569460, Valid: 8.597558\n",
      "Train: 8.530726, Valid: 8.587249\n",
      "Train: 8.527550, Valid: 8.570234\n",
      "[006/100] 5.48 sec(s) Loss: 74.094340\n",
      "[007/100] 5.32 sec(s) Loss: 73.634709\n",
      "[008/100] 5.28 sec(s) Loss: 73.270173\n",
      "[009/100] 5.28 sec(s) Loss: 72.903444\n",
      "[010/100] 5.35 sec(s) Loss: 72.639119\n",
      "Train: 8.433726, Valid: 8.557466\n",
      "[011/100] 5.28 sec(s) Loss: 72.456061\n",
      "Train: 8.417287, Valid: 8.550654\n",
      "[012/100] 5.54 sec(s) Loss: 72.079754\n",
      "[013/100] 5.28 sec(s) Loss: 71.985592\n",
      "[014/100] 5.33 sec(s) Loss: 71.752471\n",
      "[015/100] 5.29 sec(s) Loss: 71.491441\n",
      "[016/100] 5.31 sec(s) Loss: 71.321826\n",
      "[017/100] 5.47 sec(s) Loss: 71.160068\n",
      "[018/100] 5.30 sec(s) Loss: 71.012244\n",
      "[019/100] 5.46 sec(s) Loss: 70.902647\n",
      "[020/100] 5.47 sec(s) Loss: 70.711520\n",
      "[021/100] 5.91 sec(s) Loss: 70.597878\n",
      "[022/100] 5.74 sec(s) Loss: 70.447378\n",
      "Train: 9.279786, Valid: 9.214937\n",
      "Train: 8.985946, Valid: 8.923739\n",
      "Train: 8.901379, Valid: 8.854397\n",
      "Train: 8.847081, Valid: 8.797207\n",
      "[001/100] 5.74 sec(s) Loss: 87.976871\n",
      "Train: 8.807739, Valid: 8.759754\n",
      "Train: 8.750745, Valid: 8.721171\n",
      "Train: 8.722044, Valid: 8.687750\n",
      "[002/100] 5.59 sec(s) Loss: 77.913851\n",
      "Train: 8.680052, Valid: 8.666033\n",
      "Train: 8.651910, Valid: 8.652603\n",
      "[003/100] 5.42 sec(s) Loss: 76.329079\n",
      "[004/100] 5.33 sec(s) Loss: 75.482219\n",
      "Train: 8.618999, Valid: 8.647923\n",
      "Train: 8.575707, Valid: 8.607897\n",
      "Train: 8.574278, Valid: 8.600502\n",
      "[005/100] 5.42 sec(s) Loss: 74.776822\n",
      "Train: 8.561277, Valid: 8.596900\n",
      "Train: 8.550364, Valid: 8.593287\n",
      "[006/100] 5.49 sec(s) Loss: 74.266452\n",
      "Train: 8.537416, Valid: 8.590241\n",
      "Train: 8.520655, Valid: 8.572968\n",
      "[007/100] 5.41 sec(s) Loss: 73.829857\n",
      "Train: 8.499225, Valid: 8.570017\n",
      "[008/100] 5.38 sec(s) Loss: 73.458781\n",
      "[009/100] 5.41 sec(s) Loss: 73.080883\n",
      "[010/100] 5.34 sec(s) Loss: 72.752470\n",
      "Train: 8.448399, Valid: 8.567245\n",
      "Train: 8.453511, Valid: 8.554941\n",
      "[011/100] 5.36 sec(s) Loss: 72.530372\n",
      "Train: 8.429217, Valid: 8.553007\n",
      "[012/100] 5.59 sec(s) Loss: 72.250181\n",
      "Train: 8.415708, Valid: 8.545779\n",
      "[013/100] 5.34 sec(s) Loss: 72.058770\n",
      "[014/100] 5.40 sec(s) Loss: 71.881614\n",
      "[015/100] 5.34 sec(s) Loss: 71.618502\n",
      "[016/100] 5.41 sec(s) Loss: 71.482105\n",
      "[017/100] 5.55 sec(s) Loss: 71.321354\n",
      "[018/100] 5.37 sec(s) Loss: 71.164158\n",
      "[019/100] 5.35 sec(s) Loss: 71.013015\n",
      "[020/100] 5.35 sec(s) Loss: 70.863339\n",
      "[021/100] 5.40 sec(s) Loss: 70.679412\n",
      "[022/100] 5.34 sec(s) Loss: 70.605132\n",
      "[023/100] 5.57 sec(s) Loss: 70.433377\n",
      "[024/100] 5.34 sec(s) Loss: 70.352385\n",
      "Train: 9.294636, Valid: 9.231027\n",
      "Train: 9.045595, Valid: 8.978484\n",
      "Train: 8.977394, Valid: 8.935137\n",
      "Train: 8.897845, Valid: 8.870384\n",
      "[001/100] 5.45 sec(s) Loss: 88.951062\n",
      "Train: 8.858698, Valid: 8.816811\n",
      "Train: 8.803097, Valid: 8.771923\n",
      "Train: 8.766090, Valid: 8.734536\n",
      "[002/100] 5.50 sec(s) Loss: 78.186282\n",
      "Train: 8.705068, Valid: 8.694457\n",
      "Train: 8.650150, Valid: 8.674089\n",
      "[003/100] 5.39 sec(s) Loss: 76.472089\n",
      "Train: 8.633016, Valid: 8.647836\n",
      "Train: 8.617112, Valid: 8.624348\n",
      "[004/100] 5.31 sec(s) Loss: 75.488462\n",
      "[005/100] 5.35 sec(s) Loss: 74.731307\n",
      "Train: 8.548018, Valid: 8.608061\n",
      "[006/100] 5.50 sec(s) Loss: 74.219911\n",
      "Train: 8.531779, Valid: 8.597018\n",
      "Train: 8.537772, Valid: 8.595924\n",
      "[007/100] 5.46 sec(s) Loss: 73.792956\n",
      "Train: 8.517985, Valid: 8.593811\n",
      "[008/100] 5.30 sec(s) Loss: 73.361241\n",
      "Train: 8.503016, Valid: 8.593624\n",
      "Train: 8.495440, Valid: 8.577318\n",
      "Train: 8.475643, Valid: 8.573411\n",
      "[009/100] 5.44 sec(s) Loss: 73.104843\n",
      "Train: 8.473033, Valid: 8.565110\n",
      "[010/100] 5.36 sec(s) Loss: 72.791405\n",
      "[011/100] 29.83 sec(s) Loss: 72.578397\n",
      "Train: 8.427761, Valid: 8.557045\n",
      "[012/100] 5.64 sec(s) Loss: 72.279960\n",
      "[013/100] 5.35 sec(s) Loss: 71.997032\n",
      "[014/100] 5.38 sec(s) Loss: 71.859571\n",
      "Train: 8.391723, Valid: 8.556300\n",
      "[015/100] 5.32 sec(s) Loss: 71.677067\n",
      "Train: 8.399900, Valid: 8.550146\n",
      "[016/100] 5.42 sec(s) Loss: 71.479142\n",
      "[017/100] 5.60 sec(s) Loss: 71.401055\n",
      "[018/100] 5.42 sec(s) Loss: 71.162759\n",
      "[019/100] 5.35 sec(s) Loss: 70.966337\n",
      "[020/100] 5.34 sec(s) Loss: 70.846666\n",
      "[021/100] 5.36 sec(s) Loss: 70.633016\n",
      "[022/100] 5.37 sec(s) Loss: 70.617922\n",
      "[023/100] 5.56 sec(s) Loss: 70.523315\n",
      "[024/100] 5.34 sec(s) Loss: 70.335416\n",
      "[025/100] 5.38 sec(s) Loss: 70.311848\n",
      "[026/100] 5.31 sec(s) Loss: 70.115994\n",
      "[027/100] 5.38 sec(s) Loss: 69.995220\n",
      "Train: 9.287761, Valid: 9.212114\n",
      "Train: 8.979198, Valid: 8.921921\n",
      "Train: 8.888091, Valid: 8.837998\n",
      "Train: 8.817364, Valid: 8.775261\n",
      "[001/100] 7.44 sec(s) Loss: 87.455421\n",
      "Train: 8.742845, Valid: 8.724043\n",
      "Train: 8.714173, Valid: 8.711345\n",
      "Train: 8.687886, Valid: 8.685093\n",
      "[002/100] 7.31 sec(s) Loss: 77.437868\n",
      "Train: 8.678675, Valid: 8.685078\n",
      "Train: 8.624802, Valid: 8.640183\n",
      "[003/100] 7.24 sec(s) Loss: 75.511779\n",
      "Train: 8.580072, Valid: 8.614642\n",
      "Train: 8.546435, Valid: 8.600599\n",
      "Train: 8.534671, Valid: 8.590794\n",
      "[004/100] 7.24 sec(s) Loss: 74.281794\n",
      "Train: 8.509661, Valid: 8.584713\n",
      "Train: 8.488211, Valid: 8.579660\n",
      "[005/100] 7.31 sec(s) Loss: 73.394914\n",
      "Train: 8.453394, Valid: 8.572073\n",
      "Train: 8.430150, Valid: 8.563807\n",
      "[006/100] 7.77 sec(s) Loss: 72.725736\n",
      "Train: 8.414189, Valid: 8.559015\n",
      "[007/100] 7.27 sec(s) Loss: 71.933100\n",
      "Train: 8.335125, Valid: 8.534599\n",
      "[008/100] 7.21 sec(s) Loss: 71.454383\n",
      "[009/100] 7.25 sec(s) Loss: 70.771014\n",
      "[010/100] 7.45 sec(s) Loss: 70.268316\n",
      "Train: 8.275143, Valid: 8.526382\n",
      "[011/100] 7.25 sec(s) Loss: 69.788453\n",
      "[012/100] 7.74 sec(s) Loss: 69.279368\n",
      "[013/100] 7.18 sec(s) Loss: 68.884837\n",
      "[014/100] 7.24 sec(s) Loss: 68.377248\n",
      "[015/100] 7.25 sec(s) Loss: 68.069746\n",
      "[016/100] 7.24 sec(s) Loss: 67.614582\n",
      "[017/100] 7.70 sec(s) Loss: 67.206393\n",
      "[018/100] 7.24 sec(s) Loss: 66.820223\n",
      "[019/100] 7.20 sec(s) Loss: 66.542228\n",
      "[020/100] 7.22 sec(s) Loss: 66.147189\n",
      "[021/100] 7.22 sec(s) Loss: 65.745103\n",
      "[022/100] 7.22 sec(s) Loss: 65.512085\n",
      "Train: 9.174300, Valid: 9.127973\n",
      "Train: 9.015478, Valid: 8.966384\n",
      "Train: 8.897943, Valid: 8.859015\n",
      "Train: 8.860531, Valid: 8.814220\n",
      "[001/100] 7.33 sec(s) Loss: 86.998278\n",
      "Train: 8.816131, Valid: 8.782067\n",
      "Train: 8.715651, Valid: 8.697534\n",
      "Train: 8.675401, Valid: 8.668999\n",
      "[002/100] 7.26 sec(s) Loss: 77.374154\n",
      "Train: 8.593839, Valid: 8.630993\n",
      "[003/100] 7.26 sec(s) Loss: 75.558790\n",
      "Train: 8.579761, Valid: 8.627462\n",
      "Train: 8.552524, Valid: 8.611014\n",
      "[004/100] 7.44 sec(s) Loss: 74.377594\n",
      "Train: 8.512940, Valid: 8.588276\n",
      "Train: 8.493718, Valid: 8.579846\n",
      "[005/100] 7.29 sec(s) Loss: 73.527314\n",
      "Train: 8.454042, Valid: 8.578148\n",
      "Train: 8.462326, Valid: 8.576950\n",
      "[006/100] 7.72 sec(s) Loss: 72.692464\n",
      "Train: 8.420862, Valid: 8.570439\n",
      "Train: 8.408535, Valid: 8.551706\n",
      "Train: 8.388483, Valid: 8.545847\n",
      "[007/100] 7.29 sec(s) Loss: 72.137422\n",
      "Train: 8.343652, Valid: 8.530318\n",
      "[008/100] 7.20 sec(s) Loss: 71.521596\n",
      "[009/100] 7.22 sec(s) Loss: 70.909518\n",
      "[010/100] 7.18 sec(s) Loss: 70.429117\n",
      "[011/100] 7.18 sec(s) Loss: 70.007719\n",
      "[012/100] 7.75 sec(s) Loss: 69.388914\n",
      "[013/100] 7.17 sec(s) Loss: 68.988100\n",
      "[014/100] 7.25 sec(s) Loss: 68.529405\n",
      "[015/100] 7.23 sec(s) Loss: 68.145554\n",
      "[016/100] 7.63 sec(s) Loss: 67.758014\n",
      "[017/100] 7.76 sec(s) Loss: 67.317662\n",
      "[018/100] 7.26 sec(s) Loss: 66.916218\n",
      "[019/100] 7.26 sec(s) Loss: 66.643685\n",
      "Train: 9.410378, Valid: 9.362685\n",
      "Train: 8.971648, Valid: 8.927548\n",
      "Train: 8.874074, Valid: 8.830274\n",
      "Train: 8.830385, Valid: 8.792391\n",
      "[001/100] 7.30 sec(s) Loss: 87.925484\n",
      "Train: 8.804444, Valid: 8.766642\n",
      "Train: 8.725759, Valid: 8.695248\n",
      "Train: 8.712949, Valid: 8.689389\n",
      "Train: 8.694036, Valid: 8.684904\n",
      "[002/100] 7.35 sec(s) Loss: 77.325039\n",
      "Train: 8.678045, Valid: 8.664013\n",
      "Train: 8.647518, Valid: 8.634654\n",
      "Train: 8.599071, Valid: 8.607095\n",
      "[003/100] 7.25 sec(s) Loss: 75.647259\n",
      "Train: 8.581119, Valid: 8.598720\n",
      "Train: 8.552248, Valid: 8.577498\n",
      "[004/100] 7.28 sec(s) Loss: 74.554495\n",
      "Train: 8.515720, Valid: 8.572317\n",
      "Train: 8.513920, Valid: 8.565431\n",
      "[005/100] 7.22 sec(s) Loss: 73.671107\n",
      "Train: 8.456581, Valid: 8.554703\n",
      "Train: 8.450698, Valid: 8.540935\n",
      "[006/100] 7.81 sec(s) Loss: 72.876048\n",
      "Train: 8.414845, Valid: 8.536499\n",
      "[007/100] 7.25 sec(s) Loss: 72.326380\n",
      "[008/100] 7.20 sec(s) Loss: 71.709425\n",
      "Train: 8.348283, Valid: 8.527613\n",
      "[009/100] 7.30 sec(s) Loss: 71.121457\n",
      "[010/100] 7.22 sec(s) Loss: 70.520793\n",
      "Train: 8.261046, Valid: 8.517002\n",
      "[011/100] 7.29 sec(s) Loss: 70.068421\n",
      "Train: 8.215144, Valid: 8.512888\n",
      "[012/100] 7.76 sec(s) Loss: 69.641285\n",
      "[013/100] 7.28 sec(s) Loss: 69.131649\n",
      "[014/100] 7.23 sec(s) Loss: 68.717233\n",
      "[015/100] 7.30 sec(s) Loss: 68.217702\n",
      "[016/100] 7.20 sec(s) Loss: 67.892270\n",
      "[017/100] 7.73 sec(s) Loss: 67.584767\n",
      "[018/100] 7.26 sec(s) Loss: 67.154665\n",
      "[019/100] 7.22 sec(s) Loss: 66.812897\n",
      "[020/100] 7.26 sec(s) Loss: 66.525013\n",
      "[021/100] 7.21 sec(s) Loss: 66.263163\n",
      "[022/100] 7.28 sec(s) Loss: 65.885527\n",
      "[023/100] 7.75 sec(s) Loss: 65.524997\n",
      "Train: 9.261797, Valid: 9.217036\n",
      "Train: 9.114548, Valid: 9.078207\n",
      "Train: 8.849702, Valid: 8.815236\n",
      "Train: 8.788424, Valid: 8.751180\n",
      "[001/100] 12.53 sec(s) Loss: 85.593014\n",
      "Train: 8.725167, Valid: 8.684296\n",
      "Train: 8.658564, Valid: 8.662971\n",
      "[002/100] 12.42 sec(s) Loss: 76.988813\n",
      "Train: 8.660954, Valid: 8.656358\n",
      "Train: 8.542435, Valid: 8.584246\n",
      "[003/100] 12.27 sec(s) Loss: 75.027568\n",
      "Train: 8.525019, Valid: 8.578701\n",
      "Train: 8.488229, Valid: 8.557860\n",
      "Train: 8.482869, Valid: 8.556111\n",
      "[004/100] 12.28 sec(s) Loss: 73.649624\n",
      "Train: 8.417337, Valid: 8.519562\n",
      "[005/100] 12.16 sec(s) Loss: 72.634371\n",
      "Train: 8.352427, Valid: 8.509274\n",
      "[006/100] 13.56 sec(s) Loss: 71.606406\n",
      "Train: 8.296761, Valid: 8.497367\n",
      "[007/100] 12.15 sec(s) Loss: 70.693886\n",
      "[008/100] 12.22 sec(s) Loss: 69.649684\n",
      "[009/100] 12.15 sec(s) Loss: 68.859614\n",
      "[010/100] 12.13 sec(s) Loss: 67.828337\n",
      "[011/100] 12.19 sec(s) Loss: 67.062507\n",
      "[012/100] 13.52 sec(s) Loss: 66.065127\n",
      "[013/100] 12.16 sec(s) Loss: 65.159974\n",
      "[014/100] 12.17 sec(s) Loss: 64.203465\n",
      "[015/100] 12.55 sec(s) Loss: 63.419307\n",
      "[016/100] 12.29 sec(s) Loss: 62.590189\n",
      "[017/100] 13.54 sec(s) Loss: 61.845845\n",
      "[018/100] 12.13 sec(s) Loss: 60.867425\n",
      "Train: 9.235844, Valid: 9.169507\n",
      "Train: 8.991751, Valid: 8.928585\n",
      "Train: 8.778564, Valid: 8.735677\n",
      "[001/100] 12.34 sec(s) Loss: 86.304215\n",
      "Train: 8.687733, Valid: 8.666654\n",
      "Train: 8.660519, Valid: 8.650379\n",
      "[002/100] 12.30 sec(s) Loss: 76.984299\n",
      "Train: 8.623371, Valid: 8.635362\n",
      "Train: 8.603731, Valid: 8.626669\n",
      "Train: 8.611571, Valid: 8.623713\n",
      "[003/100] 12.48 sec(s) Loss: 74.988704\n",
      "Train: 8.555296, Valid: 8.600986\n",
      "Train: 8.492258, Valid: 8.577828\n",
      "[004/100] 12.22 sec(s) Loss: 73.657264\n",
      "Train: 8.420454, Valid: 8.540142\n",
      "[005/100] 12.22 sec(s) Loss: 72.531898\n",
      "Train: 8.385929, Valid: 8.522721\n",
      "Train: 8.335405, Valid: 8.521984\n",
      "[006/100] 13.54 sec(s) Loss: 71.610411\n",
      "Train: 8.302721, Valid: 8.508075\n",
      "Train: 8.271088, Valid: 8.489101\n",
      "[007/100] 12.22 sec(s) Loss: 70.588902\n",
      "[008/100] 12.21 sec(s) Loss: 69.706055\n",
      "Train: 8.156091, Valid: 8.474901\n",
      "[009/100] 12.16 sec(s) Loss: 68.658216\n",
      "[010/100] 12.18 sec(s) Loss: 67.844708\n",
      "[011/100] 12.17 sec(s) Loss: 66.955879\n",
      "[012/100] 13.44 sec(s) Loss: 66.107811\n",
      "[013/100] 12.22 sec(s) Loss: 65.270602\n",
      "[014/100] 12.16 sec(s) Loss: 64.366205\n",
      "[015/100] 12.26 sec(s) Loss: 63.550038\n",
      "[016/100] 12.18 sec(s) Loss: 62.435080\n",
      "[017/100] 13.52 sec(s) Loss: 61.736825\n",
      "[018/100] 12.22 sec(s) Loss: 60.935155\n",
      "[019/100] 12.27 sec(s) Loss: 59.973259\n",
      "[020/100] 12.19 sec(s) Loss: 59.221282\n",
      "Train: 9.171906, Valid: 9.116384\n",
      "Train: 8.944676, Valid: 8.896056\n",
      "Train: 8.871895, Valid: 8.832832\n",
      "[001/100] 12.19 sec(s) Loss: 86.315710\n",
      "Train: 8.806313, Valid: 8.785604\n",
      "Train: 8.701990, Valid: 8.694110\n",
      "Train: 8.684107, Valid: 8.671892\n",
      "[002/100] 12.29 sec(s) Loss: 76.916275\n",
      "Train: 8.615314, Valid: 8.627054\n",
      "Train: 8.582804, Valid: 8.613630\n",
      "Train: 8.536339, Valid: 8.585043\n",
      "[003/100] 12.23 sec(s) Loss: 74.960540\n",
      "Train: 8.508655, Valid: 8.582028\n",
      "Train: 8.469706, Valid: 8.549526\n",
      "[004/100] 12.27 sec(s) Loss: 73.732236\n",
      "Train: 8.430719, Valid: 8.525219\n",
      "Train: 8.395829, Valid: 8.524225\n",
      "[005/100] 12.21 sec(s) Loss: 72.546732\n",
      "Train: 8.328799, Valid: 8.507813\n",
      "[006/100] 13.58 sec(s) Loss: 71.605855\n",
      "Train: 8.273963, Valid: 8.507571\n",
      "Train: 8.261456, Valid: 8.498351\n",
      "[007/100] 12.19 sec(s) Loss: 70.539556\n",
      "[008/100] 12.20 sec(s) Loss: 69.673473\n",
      "[009/100] 12.19 sec(s) Loss: 68.807217\n",
      "Train: 8.124321, Valid: 8.495538\n",
      "[010/100] 12.15 sec(s) Loss: 67.890938\n",
      "[011/100] 12.18 sec(s) Loss: 67.033963\n",
      "Train: 7.951080, Valid: 8.482284\n",
      "[012/100] 13.47 sec(s) Loss: 66.142411\n",
      "[013/100] 12.21 sec(s) Loss: 65.242903\n",
      "[014/100] 12.16 sec(s) Loss: 64.475337\n",
      "[015/100] 12.20 sec(s) Loss: 63.492652\n",
      "[016/100] 12.14 sec(s) Loss: 62.714610\n",
      "[017/100] 13.53 sec(s) Loss: 61.810245\n",
      "[018/100] 12.27 sec(s) Loss: 60.885351\n",
      "[019/100] 12.21 sec(s) Loss: 60.126343\n",
      "[020/100] 12.19 sec(s) Loss: 59.433634\n",
      "[021/100] 12.19 sec(s) Loss: 58.480491\n",
      "[022/100] 12.18 sec(s) Loss: 57.708620\n",
      "[023/100] 13.54 sec(s) Loss: 56.868923\n"
     ]
    }
   ],
   "source": [
    "H = [45, 90, 180]\n",
    "w = [0.1, 0.2, 0.4]\n",
    "for i in H:\n",
    "    for j in w:\n",
    "        training_q4(i, j, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 45, weight_decay = 0.1, test RMSE = 8.839773\n",
      "H = 45, weight_decay = 0.2, test RMSE = 8.833174\n",
      "H = 45, weight_decay = 0.4, test RMSE = 8.821029\n",
      "H = 90, weight_decay = 0.1, test RMSE = 8.804137\n",
      "H = 90, weight_decay = 0.2, test RMSE = 8.792378\n",
      "H = 90, weight_decay = 0.4, test RMSE = 8.856186\n",
      "H = 180, weight_decay = 0.1, test RMSE = 8.815477\n",
      "H = 180, weight_decay = 0.2, test RMSE = 8.814477\n",
      "H = 180, weight_decay = 0.4, test RMSE = 8.924497\n"
     ]
    }
   ],
   "source": [
    "H = [45, 90, 180]\n",
    "w = [0.1, 0.2, 0.4]\n",
    "for i in H:\n",
    "    for j in w:\n",
    "        model = torch.load('best_H'+str(i)+'_w'+str(j)+'.pkl')\n",
    "        model.eval()\n",
    "        test_pred = model(torch.FloatTensor(X_test))\n",
    "        test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "        print('H = '+str(i)+', weight_decay = '+str(j)+', test RMSE = '+str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_q5(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(MLP_q5, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(90, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_q5(H, device):\n",
    "    model = MLP_q5(H).to(device)\n",
    "    loss = SSE()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "    num_epoch = 100\n",
    "\n",
    "    flag = 0\n",
    "    batch_cnt = 0\n",
    "    best_step_cnt = 0\n",
    "    best_rmse = math.inf\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = torch.reshape(model(data[0]), (len(data[0]), ))\n",
    "            batch_loss = loss(train_pred, data[1])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            batch_cnt += 1\n",
    "            best_step_cnt += 1\n",
    "            if batch_cnt % 100 == 0:\n",
    "                train_pred = model(torch.FloatTensor(X_subtrain))\n",
    "                train_rmse = np.sqrt(mean_squared_error(train_pred.detach().numpy(), Y_subtrain))\n",
    "                valid_pred = model(torch.FloatTensor(X_valid))\n",
    "                valid_rmse = np.sqrt(mean_squared_error(valid_pred.detach().numpy(), Y_valid))\n",
    "                if valid_rmse < best_rmse:\n",
    "                    print('Train: %f, Valid: %f' % (train_rmse, valid_rmse))\n",
    "                    best_rmse = valid_rmse\n",
    "                    best_step_cnt = 0\n",
    "                    torch.save(model, 'best_q5_H'+str(H)+'.pkl')\n",
    "            if best_step_cnt >= 5000:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Loss: %3.6f' % \\\n",
    "          (epoch + 1, num_epoch, time.time()-epoch_start_time, train_loss/train_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9.691115, Valid: 9.627952\n",
      "Train: 9.403133, Valid: 9.360952\n",
      "Train: 9.299331, Valid: 9.243391\n",
      "Train: 9.222829, Valid: 9.173450\n",
      "[001/100] 10.22 sec(s) Loss: 92.348402\n",
      "Train: 9.176925, Valid: 9.119189\n",
      "Train: 9.139065, Valid: 9.085758\n",
      "Train: 9.106014, Valid: 9.038915\n",
      "Train: 9.076671, Valid: 9.029928\n",
      "[002/100] 9.82 sec(s) Loss: 83.602306\n",
      "Train: 9.061790, Valid: 9.017046\n",
      "Train: 9.046077, Valid: 8.989340\n",
      "Train: 9.029800, Valid: 8.978065\n",
      "Train: 9.017885, Valid: 8.971027\n",
      "[003/100] 9.94 sec(s) Loss: 82.041320\n",
      "Train: 9.002901, Valid: 8.954693\n",
      "Train: 8.993168, Valid: 8.935554\n",
      "[004/100] 9.85 sec(s) Loss: 81.112333\n",
      "Train: 8.966873, Valid: 8.915466\n",
      "[005/100] 9.69 sec(s) Loss: 80.640662\n",
      "Train: 8.953935, Valid: 8.915342\n",
      "Train: 8.939194, Valid: 8.909977\n",
      "[006/100] 10.70 sec(s) Loss: 80.107932\n",
      "Train: 8.928417, Valid: 8.882907\n",
      "Train: 8.925254, Valid: 8.882612\n",
      "[007/100] 9.71 sec(s) Loss: 79.802161\n",
      "Train: 8.904022, Valid: 8.878821\n",
      "Train: 8.900240, Valid: 8.875978\n",
      "[008/100] 9.79 sec(s) Loss: 79.516106\n",
      "Train: 8.894682, Valid: 8.874462\n",
      "Train: 8.888144, Valid: 8.872406\n",
      "[009/100] 9.70 sec(s) Loss: 79.368783\n",
      "Train: 8.884335, Valid: 8.867826\n",
      "Train: 8.879762, Valid: 8.866921\n",
      "[010/100] 9.75 sec(s) Loss: 79.135467\n",
      "Train: 8.874593, Valid: 8.863103\n",
      "Train: 8.870963, Valid: 8.854367\n",
      "[011/100] 9.69 sec(s) Loss: 78.939339\n",
      "Train: 8.860036, Valid: 8.854310\n",
      "Train: 8.866386, Valid: 8.849865\n",
      "Train: 8.867760, Valid: 8.833731\n",
      "[012/100] 10.69 sec(s) Loss: 78.797779\n",
      "[013/100] 9.72 sec(s) Loss: 78.780056\n",
      "[014/100] 9.71 sec(s) Loss: 78.679010\n",
      "[015/100] 9.75 sec(s) Loss: 78.375093\n",
      "[016/100] 9.73 sec(s) Loss: 78.519522\n",
      "[017/100] 10.72 sec(s) Loss: 78.289844\n",
      "[018/100] 9.76 sec(s) Loss: 78.270453\n",
      "Train: 8.815522, Valid: 8.825354\n",
      "[019/100] 9.75 sec(s) Loss: 78.230308\n",
      "Train: 8.817457, Valid: 8.823579\n",
      "[020/100] 9.73 sec(s) Loss: 78.137745\n",
      "Train: 8.816817, Valid: 8.823192\n",
      "[021/100] 9.78 sec(s) Loss: 77.958200\n",
      "[022/100] 9.71 sec(s) Loss: 77.912401\n",
      "Train: 8.811864, Valid: 8.820395\n",
      "[023/100] 10.73 sec(s) Loss: 77.965095\n",
      "Train: 8.798269, Valid: 8.818533\n",
      "[024/100] 9.77 sec(s) Loss: 77.902829\n",
      "[025/100] 9.72 sec(s) Loss: 77.720882\n",
      "Train: 8.794625, Valid: 8.816202\n",
      "[026/100] 9.80 sec(s) Loss: 77.700437\n",
      "[027/100] 9.76 sec(s) Loss: 77.678503\n",
      "[028/100] 10.72 sec(s) Loss: 77.582225\n",
      "[029/100] 9.74 sec(s) Loss: 77.573613\n",
      "[030/100] 9.76 sec(s) Loss: 77.498026\n",
      "Train: 8.791710, Valid: 8.815631\n",
      "[031/100] 9.72 sec(s) Loss: 77.697077\n",
      "[032/100] 9.75 sec(s) Loss: 77.407067\n",
      "[033/100] 9.80 sec(s) Loss: 77.396918\n",
      "Train: 8.780729, Valid: 8.812428\n",
      "[034/100] 10.69 sec(s) Loss: 77.344074\n",
      "[035/100] 9.93 sec(s) Loss: 77.432291\n",
      "Train: 8.782303, Valid: 8.808648\n",
      "[036/100] 9.91 sec(s) Loss: 77.274285\n",
      "[037/100] 10.23 sec(s) Loss: 77.384796\n",
      "[038/100] 10.03 sec(s) Loss: 77.061553\n",
      "Train: 8.766914, Valid: 8.805173\n",
      "[039/100] 11.44 sec(s) Loss: 76.967506\n",
      "[040/100] 9.99 sec(s) Loss: 77.309202\n",
      "[041/100] 9.80 sec(s) Loss: 77.032944\n",
      "[042/100] 10.15 sec(s) Loss: 77.110718\n",
      "[043/100] 9.87 sec(s) Loss: 77.214053\n",
      "[044/100] 9.81 sec(s) Loss: 77.092307\n",
      "[045/100] 10.68 sec(s) Loss: 77.019580\n",
      "Train: 8.750540, Valid: 8.800784\n",
      "[046/100] 9.83 sec(s) Loss: 77.059417\n",
      "[047/100] 9.92 sec(s) Loss: 77.066173\n",
      "[048/100] 9.97 sec(s) Loss: 76.927075\n",
      "[049/100] 9.77 sec(s) Loss: 76.915908\n",
      "[050/100] 10.93 sec(s) Loss: 77.036941\n",
      "[051/100] 10.30 sec(s) Loss: 76.848397\n",
      "[052/100] 10.20 sec(s) Loss: 76.825747\n",
      "[053/100] 10.37 sec(s) Loss: 76.933893\n",
      "[054/100] 10.02 sec(s) Loss: 76.905804\n",
      "[055/100] 9.88 sec(s) Loss: 76.928682\n",
      "[056/100] 10.94 sec(s) Loss: 76.626506\n",
      "Train: 8.742091, Valid: 8.795147\n",
      "[057/100] 9.84 sec(s) Loss: 76.761199\n",
      "[058/100] 9.92 sec(s) Loss: 76.697950\n",
      "[059/100] 10.04 sec(s) Loss: 76.605259\n",
      "[060/100] 9.81 sec(s) Loss: 76.762246\n",
      "[061/100] 9.80 sec(s) Loss: 76.736589\n",
      "[062/100] 10.98 sec(s) Loss: 76.610350\n",
      "[063/100] 9.87 sec(s) Loss: 76.766295\n",
      "[064/100] 9.85 sec(s) Loss: 76.681222\n",
      "[065/100] 9.70 sec(s) Loss: 76.785478\n",
      "Train: 8.737926, Valid: 8.787536\n",
      "[066/100] 9.79 sec(s) Loss: 76.649388\n",
      "[067/100] 10.70 sec(s) Loss: 76.700362\n",
      "[068/100] 9.75 sec(s) Loss: 76.593674\n",
      "[069/100] 9.78 sec(s) Loss: 76.662686\n",
      "[070/100] 9.73 sec(s) Loss: 76.559460\n",
      "Train: 8.734443, Valid: 8.787211\n",
      "[071/100] 9.81 sec(s) Loss: 76.478718\n",
      "[072/100] 9.75 sec(s) Loss: 76.507623\n",
      "[073/100] 11.14 sec(s) Loss: 76.595499\n",
      "[074/100] 9.74 sec(s) Loss: 76.499924\n",
      "[075/100] 9.87 sec(s) Loss: 76.566134\n",
      "[076/100] 9.74 sec(s) Loss: 76.398424\n",
      "[077/100] 9.76 sec(s) Loss: 76.522570\n",
      "[078/100] 10.70 sec(s) Loss: 76.442577\n",
      "[079/100] 9.77 sec(s) Loss: 76.458815\n",
      "Train: 8.724599, Valid: 8.786972\n",
      "[080/100] 9.80 sec(s) Loss: 76.331156\n",
      "[081/100] 9.74 sec(s) Loss: 76.364671\n",
      "[082/100] 9.79 sec(s) Loss: 76.493517\n",
      "[083/100] 9.74 sec(s) Loss: 76.398888\n",
      "[084/100] 10.74 sec(s) Loss: 76.351379\n",
      "[085/100] 9.74 sec(s) Loss: 76.409316\n",
      "[086/100] 9.94 sec(s) Loss: 76.438092\n",
      "[087/100] 9.79 sec(s) Loss: 76.320209\n",
      "[088/100] 10.40 sec(s) Loss: 76.263968\n",
      "[089/100] 10.91 sec(s) Loss: 76.225417\n",
      "[090/100] 9.91 sec(s) Loss: 76.315868\n",
      "[091/100] 10.47 sec(s) Loss: 76.212890\n"
     ]
    }
   ],
   "source": [
    "training_q5(90, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 180, test RMSE = 8.773399\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_q5_H90.pkl')\n",
    "model.eval()\n",
    "test_pred = model(torch.FloatTensor(X_test))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "print('H = '+str(i)+', test RMSE = '+str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10.611014, Valid: 10.576477\n",
      "Train: 10.037332, Valid: 9.998349\n",
      "Train: 9.801664, Valid: 9.760788\n",
      "Train: 9.698590, Valid: 9.669986\n",
      "[001/100] 5.25 sec(s) Loss: 103.827231\n",
      "Train: 9.646609, Valid: 9.597734\n",
      "Train: 9.602829, Valid: 9.538258\n",
      "Train: 9.550961, Valid: 9.504905\n",
      "[002/100] 5.11 sec(s) Loss: 92.321439\n",
      "Train: 9.539949, Valid: 9.455110\n",
      "Train: 9.494735, Valid: 9.435950\n",
      "[003/100] 5.11 sec(s) Loss: 90.562939\n",
      "Train: 9.485288, Valid: 9.427824\n",
      "Train: 9.478751, Valid: 9.422654\n",
      "Train: 9.465181, Valid: 9.414675\n",
      "Train: 9.461243, Valid: 9.409054\n",
      "[004/100] 5.12 sec(s) Loss: 89.946521\n",
      "Train: 9.452930, Valid: 9.383854\n",
      "Train: 9.449419, Valid: 9.380079\n",
      "[005/100] 5.02 sec(s) Loss: 89.192891\n",
      "Train: 9.424513, Valid: 9.369101\n",
      "Train: 9.418166, Valid: 9.352162\n",
      "[006/100] 5.23 sec(s) Loss: 88.937700\n",
      "Train: 9.419656, Valid: 9.338206\n",
      "Train: 9.412177, Valid: 9.337986\n",
      "[007/100] 5.01 sec(s) Loss: 88.714940\n",
      "Train: 9.397417, Valid: 9.334004\n",
      "Train: 9.399323, Valid: 9.310922\n",
      "[008/100] 5.12 sec(s) Loss: 88.528425\n",
      "[009/100] 5.02 sec(s) Loss: 88.363542\n",
      "[010/100] 4.99 sec(s) Loss: 88.104110\n",
      "[011/100] 5.06 sec(s) Loss: 88.075102\n",
      "Train: 9.378145, Valid: 9.309494\n",
      "[012/100] 5.18 sec(s) Loss: 88.095763\n",
      "Train: 9.361644, Valid: 9.308725\n",
      "[013/100] 5.07 sec(s) Loss: 88.087403\n",
      "Train: 9.365689, Valid: 9.307036\n",
      "[014/100] 5.01 sec(s) Loss: 87.695038\n",
      "Train: 9.360623, Valid: 9.303720\n",
      "Train: 9.372806, Valid: 9.300166\n",
      "[015/100] 5.06 sec(s) Loss: 87.853873\n",
      "Train: 9.371912, Valid: 9.296662\n",
      "[016/100] 5.02 sec(s) Loss: 87.687659\n",
      "Train: 9.351616, Valid: 9.288229\n",
      "Train: 9.357036, Valid: 9.276993\n",
      "[017/100] 5.28 sec(s) Loss: 87.736594\n",
      "[018/100] 4.99 sec(s) Loss: 87.644383\n",
      "Train: 9.346855, Valid: 9.273501\n",
      "[019/100] 5.01 sec(s) Loss: 87.477446\n",
      "[020/100] 5.04 sec(s) Loss: 87.491141\n",
      "[021/100] 5.01 sec(s) Loss: 87.366097\n",
      "[022/100] 5.05 sec(s) Loss: 87.296108\n",
      "[023/100] 5.25 sec(s) Loss: 87.323433\n",
      "[024/100] 5.04 sec(s) Loss: 87.421049\n",
      "[025/100] 5.00 sec(s) Loss: 87.401760\n",
      "[026/100] 5.03 sec(s) Loss: 87.364374\n",
      "Train: 9.341116, Valid: 9.273443\n",
      "[027/100] 5.03 sec(s) Loss: 87.207267\n",
      "Train: 9.335537, Valid: 9.270464\n",
      "[028/100] 5.20 sec(s) Loss: 87.275264\n",
      "Train: 9.342125, Valid: 9.259276\n",
      "Train: 9.338711, Valid: 9.255785\n",
      "[029/100] 5.08 sec(s) Loss: 86.990352\n",
      "[030/100] 4.97 sec(s) Loss: 87.275866\n",
      "[031/100] 5.04 sec(s) Loss: 87.146111\n",
      "[032/100] 5.03 sec(s) Loss: 87.066043\n",
      "[033/100] 5.05 sec(s) Loss: 87.215164\n",
      "[034/100] 5.18 sec(s) Loss: 87.104605\n",
      "[035/100] 5.16 sec(s) Loss: 87.213525\n",
      "[036/100] 5.03 sec(s) Loss: 86.956690\n",
      "[037/100] 5.15 sec(s) Loss: 87.093201\n",
      "[038/100] 5.93 sec(s) Loss: 87.217638\n",
      "[039/100] 5.82 sec(s) Loss: 87.126686\n",
      "[040/100] 5.06 sec(s) Loss: 87.188492\n",
      "Train: 10.012170, Valid: 9.974981\n",
      "Train: 9.648360, Valid: 9.586791\n",
      "Train: 9.511625, Valid: 9.430549\n",
      "Train: 9.441887, Valid: 9.392513\n",
      "[001/100] 7.25 sec(s) Loss: 97.248284\n",
      "Train: 9.392164, Valid: 9.319847\n",
      "Train: 9.353495, Valid: 9.301967\n",
      "Train: 9.319185, Valid: 9.252414\n",
      "Train: 9.286882, Valid: 9.236698\n",
      "[002/100] 6.95 sec(s) Loss: 87.387629\n",
      "Train: 9.257526, Valid: 9.198831\n",
      "Train: 9.233430, Valid: 9.173254\n",
      "[003/100] 6.85 sec(s) Loss: 85.597066\n",
      "Train: 9.212729, Valid: 9.144238\n",
      "Train: 9.182775, Valid: 9.130624\n",
      "[004/100] 6.89 sec(s) Loss: 84.768775\n",
      "Train: 9.164475, Valid: 9.116523\n",
      "Train: 9.160151, Valid: 9.109728\n",
      "[005/100] 6.90 sec(s) Loss: 84.208666\n",
      "Train: 9.141280, Valid: 9.097096\n",
      "Train: 9.130314, Valid: 9.085636\n",
      "[006/100] 7.27 sec(s) Loss: 83.717294\n",
      "Train: 9.118584, Valid: 9.081929\n",
      "[007/100] 6.90 sec(s) Loss: 83.476102\n",
      "Train: 9.116673, Valid: 9.072989\n",
      "Train: 9.107133, Valid: 9.055339\n",
      "[008/100] 6.84 sec(s) Loss: 83.049852\n",
      "Train: 9.107306, Valid: 9.046948\n",
      "[009/100] 6.91 sec(s) Loss: 82.953081\n",
      "[010/100] 6.84 sec(s) Loss: 82.927322\n",
      "[011/100] 6.89 sec(s) Loss: 82.548422\n",
      "Train: 9.072055, Valid: 9.042774\n",
      "[012/100] 7.32 sec(s) Loss: 82.587516\n",
      "Train: 9.070585, Valid: 9.011539\n",
      "[013/100] 6.87 sec(s) Loss: 82.389407\n",
      "[014/100] 6.88 sec(s) Loss: 82.366271\n",
      "[015/100] 6.85 sec(s) Loss: 82.293077\n",
      "Train: 9.063092, Valid: 9.006293\n",
      "[016/100] 7.14 sec(s) Loss: 82.178185\n",
      "[017/100] 7.66 sec(s) Loss: 82.185780\n",
      "[018/100] 6.98 sec(s) Loss: 81.998517\n",
      "[019/100] 6.88 sec(s) Loss: 81.943701\n",
      "[020/100] 6.87 sec(s) Loss: 82.118320\n",
      "Train: 9.041105, Valid: 8.996014\n",
      "[021/100] 6.91 sec(s) Loss: 81.916410\n",
      "[022/100] 6.86 sec(s) Loss: 81.876296\n",
      "Train: 9.028656, Valid: 8.973864\n",
      "[023/100] 7.32 sec(s) Loss: 81.666344\n",
      "[024/100] 6.88 sec(s) Loss: 81.617834\n",
      "[025/100] 6.88 sec(s) Loss: 81.842000\n",
      "[026/100] 6.85 sec(s) Loss: 81.658058\n",
      "[027/100] 6.92 sec(s) Loss: 81.612628\n",
      "[028/100] 7.31 sec(s) Loss: 81.615382\n",
      "[029/100] 6.85 sec(s) Loss: 81.500679\n",
      "[030/100] 6.91 sec(s) Loss: 81.591237\n",
      "[031/100] 6.89 sec(s) Loss: 81.505160\n",
      "[032/100] 7.07 sec(s) Loss: 81.481102\n",
      "[033/100] 7.07 sec(s) Loss: 81.418579\n",
      "[034/100] 7.32 sec(s) Loss: 81.312895\n",
      "Train: 9.500845, Valid: 9.501787\n",
      "Train: 9.288208, Valid: 9.251475\n",
      "Train: 9.169091, Valid: 9.105445\n",
      "Train: 9.110439, Valid: 9.059558\n",
      "[001/100] 18.40 sec(s) Loss: 89.480677\n",
      "Train: 9.055487, Valid: 9.018963\n",
      "Train: 9.020103, Valid: 8.960814\n",
      "Train: 8.983373, Valid: 8.951592\n",
      "Train: 8.964340, Valid: 8.923738\n",
      "[002/100] 16.22 sec(s) Loss: 81.544536\n",
      "Train: 8.946606, Valid: 8.921916\n",
      "Train: 8.920366, Valid: 8.895489\n",
      "Train: 8.914973, Valid: 8.859126\n",
      "[003/100] 16.10 sec(s) Loss: 79.884704\n",
      "Train: 8.888275, Valid: 8.852763\n",
      "Train: 8.875204, Valid: 8.849524\n",
      "Train: 8.859202, Valid: 8.847071\n",
      "[004/100] 16.15 sec(s) Loss: 79.012718\n",
      "Train: 8.838081, Valid: 8.818976\n",
      "Train: 8.842925, Valid: 8.814805\n",
      "Train: 8.836559, Valid: 8.811123\n",
      "[005/100] 16.17 sec(s) Loss: 78.323051\n",
      "Train: 8.811356, Valid: 8.807359\n",
      "Train: 8.803658, Valid: 8.788731\n",
      "[006/100] 18.12 sec(s) Loss: 77.803564\n",
      "Train: 8.788945, Valid: 8.786908\n",
      "Train: 8.781495, Valid: 8.783384\n",
      "Train: 8.766055, Valid: 8.762976\n",
      "Train: 8.762794, Valid: 8.761021\n",
      "[007/100] 16.14 sec(s) Loss: 77.330591\n",
      "[008/100] 16.08 sec(s) Loss: 77.126488\n",
      "Train: 8.735668, Valid: 8.759095\n",
      "Train: 8.744299, Valid: 8.751451\n",
      "[009/100] 16.20 sec(s) Loss: 76.667792\n",
      "Train: 8.727806, Valid: 8.750300\n",
      "[010/100] 16.11 sec(s) Loss: 76.638178\n",
      "Train: 8.710420, Valid: 8.732341\n",
      "[011/100] 16.16 sec(s) Loss: 76.185772\n",
      "[012/100] 18.17 sec(s) Loss: 76.126113\n",
      "Train: 8.690417, Valid: 8.732034\n",
      "[013/100] 16.14 sec(s) Loss: 75.961717\n",
      "Train: 8.679230, Valid: 8.718944\n",
      "[014/100] 16.16 sec(s) Loss: 75.846069\n",
      "[015/100] 16.13 sec(s) Loss: 75.622006\n",
      "[016/100] 16.15 sec(s) Loss: 75.312321\n",
      "Train: 8.645679, Valid: 8.701996\n",
      "[017/100] 18.11 sec(s) Loss: 75.329413\n",
      "[018/100] 16.13 sec(s) Loss: 75.019893\n",
      "[019/100] 16.14 sec(s) Loss: 75.129528\n",
      "[020/100] 16.28 sec(s) Loss: 74.947956\n",
      "Train: 8.617654, Valid: 8.701520\n",
      "[021/100] 16.40 sec(s) Loss: 74.943576\n",
      "[022/100] 16.07 sec(s) Loss: 74.701483\n",
      "[023/100] 18.14 sec(s) Loss: 74.621155\n",
      "[024/100] 16.04 sec(s) Loss: 74.447499\n",
      "[025/100] 16.17 sec(s) Loss: 74.402009\n",
      "[026/100] 16.17 sec(s) Loss: 74.401019\n",
      "[027/100] 16.19 sec(s) Loss: 74.364529\n",
      "[028/100] 18.16 sec(s) Loss: 74.161249\n",
      "[029/100] 16.17 sec(s) Loss: 74.164099\n",
      "[030/100] 16.17 sec(s) Loss: 74.065909\n",
      "[031/100] 16.13 sec(s) Loss: 74.207733\n",
      "[032/100] 16.18 sec(s) Loss: 74.015698\n",
      "Train: 9.370392, Valid: 9.300429\n",
      "Train: 9.173728, Valid: 9.111422\n",
      "Train: 9.085677, Valid: 9.031795\n",
      "Train: 9.011895, Valid: 8.962994\n",
      "[001/100] 41.65 sec(s) Loss: 87.314100\n",
      "Train: 8.995409, Valid: 8.962083\n",
      "Train: 8.944427, Valid: 8.917022\n",
      "Train: 8.904206, Valid: 8.861684\n",
      "Train: 8.879647, Valid: 8.848327\n",
      "[002/100] 36.83 sec(s) Loss: 80.208680\n",
      "Train: 8.838294, Valid: 8.835704\n",
      "Train: 8.820084, Valid: 8.807482\n",
      "Train: 8.803983, Valid: 8.786449\n",
      "[003/100] 55.04 sec(s) Loss: 78.578177\n",
      "Train: 8.765242, Valid: 8.752975\n",
      "[004/100] 1375.08 sec(s) Loss: 77.473348\n",
      "Train: 8.718041, Valid: 8.749491\n",
      "Train: 8.717602, Valid: 8.734018\n",
      "[005/100] 37.73 sec(s) Loss: 76.727132\n",
      "Train: 8.701666, Valid: 8.713887\n",
      "Train: 8.685675, Valid: 8.706443\n",
      "[006/100] 42.36 sec(s) Loss: 76.171496\n",
      "Train: 8.668131, Valid: 8.697387\n",
      "[007/100] 37.10 sec(s) Loss: 75.659494\n",
      "Train: 8.621327, Valid: 8.675278\n",
      "[008/100] 37.02 sec(s) Loss: 75.257654\n",
      "[009/100] 37.18 sec(s) Loss: 74.882813\n",
      "Train: 8.598361, Valid: 8.671290\n",
      "[010/100] 37.50 sec(s) Loss: 74.400180\n",
      "Train: 8.575572, Valid: 8.668851\n",
      "[011/100] 36.44 sec(s) Loss: 74.098094\n",
      "Train: 8.552066, Valid: 8.654847\n",
      "[012/100] 41.02 sec(s) Loss: 73.766647\n",
      "Train: 8.533596, Valid: 8.654821\n",
      "[013/100] 35.88 sec(s) Loss: 73.455709\n",
      "[014/100] 35.63 sec(s) Loss: 73.351401\n",
      "[015/100] 35.95 sec(s) Loss: 73.115665\n",
      "[016/100] 35.66 sec(s) Loss: 72.848799\n",
      "Train: 8.484920, Valid: 8.650490\n",
      "[017/100] 41.71 sec(s) Loss: 72.628751\n",
      "[018/100] 35.68 sec(s) Loss: 72.467711\n",
      "[019/100] 35.69 sec(s) Loss: 72.376696\n",
      "[020/100] 35.66 sec(s) Loss: 72.068054\n",
      "Train: 8.418910, Valid: 8.620111\n",
      "[021/100] 36.11 sec(s) Loss: 71.808448\n",
      "[022/100] 35.81 sec(s) Loss: 71.583459\n",
      "[023/100] 40.99 sec(s) Loss: 71.483314\n",
      "[024/100] 35.92 sec(s) Loss: 71.443063\n",
      "[025/100] 35.82 sec(s) Loss: 71.298487\n",
      "[026/100] 35.92 sec(s) Loss: 70.994470\n",
      "[027/100] 36.22 sec(s) Loss: 70.807521\n",
      "[028/100] 41.09 sec(s) Loss: 70.662084\n",
      "[029/100] 36.10 sec(s) Loss: 70.551796\n",
      "[030/100] 36.07 sec(s) Loss: 70.365329\n",
      "[031/100] 35.93 sec(s) Loss: 70.313406\n",
      "[032/100] 35.89 sec(s) Loss: 70.260686\n"
     ]
    }
   ],
   "source": [
    "H = [20, 45, 180, 360]\n",
    "for i in H:\n",
    "    training_q5(i, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 20, test RMSE = 9.038035\n",
      "H = 45, test RMSE = 8.865167\n",
      "H = 180, test RMSE = 8.751879\n",
      "H = 360, test RMSE = 8.753275\n"
     ]
    }
   ],
   "source": [
    "for i in H:\n",
    "    model = torch.load('best_q5_H'+str(i)+'.pkl')\n",
    "    model.eval()\n",
    "    test_pred = model(torch.FloatTensor(X_test))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "    print('H = '+str(i)+', test RMSE = '+str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2_L1(nn.Module):\n",
    "    def __init__(self, z):\n",
    "        super().__init__()\n",
    "        self.z = z\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        return self.z * torch.sum(torch.pow((x - y), 2)) + (1 - self.z) * torch.sum(torch.abs(x - y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_q7(z, device):\n",
    "    model = MLP_q5(90).to(device)\n",
    "    loss = L2_L1(z)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "    num_epoch = 100\n",
    "\n",
    "    flag = 0\n",
    "    batch_cnt = 0\n",
    "    best_step_cnt = 0\n",
    "    best_rmse = math.inf\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = torch.reshape(model(data[0]), (len(data[0]), ))\n",
    "            batch_loss = loss(train_pred, data[1])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += batch_loss.item()\n",
    "            \n",
    "\n",
    "            batch_cnt += 1\n",
    "            best_step_cnt += 1\n",
    "            if batch_cnt % 100 == 0:\n",
    "                train_pred = model(torch.FloatTensor(X_subtrain))\n",
    "                train_rmse = np.sqrt(mean_squared_error(train_pred.detach().numpy(), Y_subtrain))\n",
    "                valid_pred = model(torch.FloatTensor(X_valid))\n",
    "                valid_rmse = np.sqrt(mean_squared_error(valid_pred.detach().numpy(), Y_valid))\n",
    "                if valid_rmse < best_rmse:\n",
    "                    print('Train: %f, Valid: %f' % (train_rmse, valid_rmse))\n",
    "                    best_rmse = valid_rmse\n",
    "                    best_step_cnt = 0\n",
    "                    torch.save(model, 'best_q7_z'+str(z)+'.pkl')\n",
    "            if best_step_cnt >= 5000:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Loss: %3.6f' % \\\n",
    "          (epoch + 1, num_epoch, time.time()-epoch_start_time, train_loss/train_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10.696766, Valid: 10.671568\n",
      "Train: 9.926058, Valid: 9.889107\n",
      "Train: 9.692993, Valid: 9.645587\n",
      "Train: 9.550718, Valid: 9.500651\n",
      "[001/100] 9.76 sec(s) Loss: 6.764667\n",
      "Train: 9.525843, Valid: 9.486466\n",
      "Train: 9.432894, Valid: 9.376958\n",
      "Train: 9.424075, Valid: 9.374808\n",
      "Train: 9.373528, Valid: 9.350177\n",
      "[002/100] 9.74 sec(s) Loss: 6.201927\n",
      "Train: 9.327116, Valid: 9.292807\n",
      "Train: 9.328636, Valid: 9.285329\n",
      "[003/100] 9.70 sec(s) Loss: 6.099921\n",
      "Train: 9.252521, Valid: 9.214709\n",
      "Train: 9.263678, Valid: 9.203506\n",
      "[004/100] 9.73 sec(s) Loss: 6.057398\n",
      "[005/100] 9.66 sec(s) Loss: 6.027022\n",
      "[006/100] 10.60 sec(s) Loss: 5.999294\n",
      "[007/100] 9.63 sec(s) Loss: 5.986274\n",
      "Train: 9.196467, Valid: 9.170694\n",
      "[008/100] 9.87 sec(s) Loss: 5.970306\n",
      "[009/100] 9.67 sec(s) Loss: 5.961331\n",
      "[010/100] 9.71 sec(s) Loss: 5.950344\n",
      "Train: 9.174382, Valid: 9.160086\n",
      "[011/100] 9.78 sec(s) Loss: 5.939454\n",
      "[012/100] 10.60 sec(s) Loss: 5.935695\n",
      "[013/100] 9.68 sec(s) Loss: 5.925118\n",
      "[014/100] 9.68 sec(s) Loss: 5.928199\n",
      "[015/100] 9.77 sec(s) Loss: 5.917781\n",
      "[016/100] 9.72 sec(s) Loss: 5.912520\n",
      "Train: 9.150453, Valid: 9.140653\n",
      "[017/100] 10.68 sec(s) Loss: 5.908582\n",
      "[018/100] 9.65 sec(s) Loss: 5.905996\n",
      "[019/100] 9.79 sec(s) Loss: 5.903158\n",
      "[020/100] 9.90 sec(s) Loss: 5.899435\n",
      "[021/100] 9.70 sec(s) Loss: 5.894833\n",
      "[022/100] 9.76 sec(s) Loss: 5.889197\n",
      "[023/100] 10.64 sec(s) Loss: 5.887642\n",
      "[024/100] 9.73 sec(s) Loss: 5.885011\n",
      "[025/100] 9.69 sec(s) Loss: 5.883524\n",
      "[026/100] 9.76 sec(s) Loss: 5.878827\n",
      "Train: 9.127087, Valid: 9.134166\n",
      "[027/100] 9.74 sec(s) Loss: 5.882763\n",
      "[028/100] 10.62 sec(s) Loss: 5.877053\n",
      "[029/100] 9.72 sec(s) Loss: 5.871519\n",
      "[030/100] 9.70 sec(s) Loss: 5.874528\n",
      "[031/100] 9.74 sec(s) Loss: 5.866532\n",
      "Train: 9.128645, Valid: 9.119425\n",
      "[032/100] 9.81 sec(s) Loss: 5.866303\n",
      "[033/100] 9.77 sec(s) Loss: 5.871637\n",
      "[034/100] 10.66 sec(s) Loss: 5.868139\n",
      "[035/100] 9.77 sec(s) Loss: 5.859886\n",
      "[036/100] 9.67 sec(s) Loss: 5.861205\n",
      "[037/100] 9.69 sec(s) Loss: 5.858943\n",
      "[038/100] 9.77 sec(s) Loss: 5.857253\n",
      "[039/100] 11.16 sec(s) Loss: 5.854903\n",
      "[040/100] 9.81 sec(s) Loss: 5.859667\n",
      "[041/100] 9.70 sec(s) Loss: 5.852984\n",
      "[042/100] 9.76 sec(s) Loss: 5.857850\n",
      "[043/100] 9.70 sec(s) Loss: 5.854808\n",
      "Train: 9.767895, Valid: 9.719329\n",
      "Train: 9.435563, Valid: 9.387841\n",
      "Train: 9.350592, Valid: 9.286180\n",
      "Train: 9.227719, Valid: 9.190471\n",
      "[001/100] 9.76 sec(s) Loss: 15.412426\n",
      "Train: 9.169414, Valid: 9.089631\n",
      "Train: 9.133703, Valid: 9.074209\n",
      "Train: 9.090356, Valid: 9.032966\n",
      "[002/100] 9.86 sec(s) Loss: 14.047158\n",
      "Train: 9.075784, Valid: 9.024413\n",
      "Train: 9.045259, Valid: 8.999880\n",
      "Train: 9.015653, Valid: 8.980066\n",
      "[003/100] 9.76 sec(s) Loss: 13.820634\n",
      "Train: 9.003215, Valid: 8.954699\n",
      "[004/100] 9.78 sec(s) Loss: 13.709878\n",
      "Train: 8.974363, Valid: 8.944581\n",
      "Train: 8.976039, Valid: 8.939056\n",
      "[005/100] 9.75 sec(s) Loss: 13.609213\n",
      "Train: 8.953626, Valid: 8.925855\n",
      "[006/100] 10.67 sec(s) Loss: 13.560466\n",
      "Train: 8.938545, Valid: 8.905890\n",
      "Train: 8.926248, Valid: 8.896734\n",
      "Train: 8.930199, Valid: 8.895309\n",
      "[007/100] 9.81 sec(s) Loss: 13.514713\n",
      "[008/100] 9.72 sec(s) Loss: 13.460061\n",
      "Train: 8.917358, Valid: 8.888483\n",
      "[009/100] 9.77 sec(s) Loss: 13.452078\n",
      "[010/100] 9.73 sec(s) Loss: 13.403941\n",
      "Train: 8.881574, Valid: 8.883702\n",
      "Train: 8.888016, Valid: 8.863102\n",
      "[011/100] 9.78 sec(s) Loss: 13.393989\n",
      "Train: 8.869687, Valid: 8.850858\n",
      "[012/100] 10.68 sec(s) Loss: 13.356681\n",
      "[013/100] 9.72 sec(s) Loss: 13.367704\n",
      "[014/100] 9.75 sec(s) Loss: 13.336864\n",
      "[015/100] 9.67 sec(s) Loss: 13.322534\n",
      "[016/100] 9.77 sec(s) Loss: 13.315237\n",
      "Train: 8.852233, Valid: 8.841722\n",
      "[017/100] 10.71 sec(s) Loss: 13.294683\n",
      "[018/100] 9.76 sec(s) Loss: 13.265343\n",
      "[019/100] 9.72 sec(s) Loss: 13.284590\n",
      "[020/100] 9.87 sec(s) Loss: 13.267955\n",
      "[021/100] 9.75 sec(s) Loss: 13.273758\n",
      "[022/100] 9.78 sec(s) Loss: 13.242520\n",
      "[023/100] 10.82 sec(s) Loss: 13.245992\n",
      "Train: 8.816944, Valid: 8.835551\n",
      "[024/100] 9.70 sec(s) Loss: 13.239819\n",
      "[025/100] 15.81 sec(s) Loss: 13.239044\n",
      "[026/100] 9.75 sec(s) Loss: 13.220752\n",
      "[027/100] 9.79 sec(s) Loss: 13.222930\n",
      "[028/100] 10.65 sec(s) Loss: 13.212640\n",
      "[029/100] 9.77 sec(s) Loss: 13.200822\n",
      "Train: 8.811901, Valid: 8.834904\n",
      "[030/100] 9.70 sec(s) Loss: 13.197179\n",
      "[031/100] 9.77 sec(s) Loss: 13.185661\n",
      "Train: 8.800443, Valid: 8.822500\n",
      "[032/100] 9.81 sec(s) Loss: 13.198362\n",
      "[033/100] 9.75 sec(s) Loss: 13.167266\n",
      "[034/100] 10.75 sec(s) Loss: 13.175645\n",
      "[035/100] 9.71 sec(s) Loss: 13.187699\n",
      "[036/100] 9.73 sec(s) Loss: 13.167509\n",
      "[037/100] 9.69 sec(s) Loss: 13.162633\n",
      "[038/100] 9.78 sec(s) Loss: 13.152163\n",
      "[039/100] 10.68 sec(s) Loss: 13.150867\n",
      "[040/100] 9.68 sec(s) Loss: 13.124506\n",
      "[041/100] 9.72 sec(s) Loss: 13.147953\n",
      "[042/100] 9.66 sec(s) Loss: 13.139181\n",
      "[043/100] 9.76 sec(s) Loss: 13.150946\n",
      "Train: 9.745631, Valid: 9.698467\n",
      "Train: 9.456740, Valid: 9.403525\n",
      "Train: 9.320858, Valid: 9.258667\n",
      "Train: 9.230284, Valid: 9.167018\n",
      "[001/100] 9.80 sec(s) Loss: 49.985163\n",
      "Train: 9.184983, Valid: 9.147311\n",
      "Train: 9.148218, Valid: 9.089822\n",
      "Train: 9.107383, Valid: 9.057961\n",
      "Train: 9.078149, Valid: 9.042963\n",
      "[002/100] 9.74 sec(s) Loss: 44.997673\n",
      "Train: 9.061868, Valid: 8.992345\n",
      "Train: 9.031190, Valid: 8.981197\n",
      "[003/100] 9.76 sec(s) Loss: 44.160093\n",
      "Train: 9.009177, Valid: 8.964700\n",
      "Train: 9.001878, Valid: 8.952947\n",
      "Train: 8.995064, Valid: 8.947399\n",
      "Train: 8.982408, Valid: 8.943989\n",
      "[004/100] 9.74 sec(s) Loss: 43.681382\n",
      "Train: 8.984214, Valid: 8.929317\n",
      "Train: 8.949950, Valid: 8.908268\n",
      "[005/100] 9.79 sec(s) Loss: 43.410127\n",
      "Train: 8.932594, Valid: 8.904156\n",
      "Train: 8.925278, Valid: 8.900798\n",
      "[006/100] 10.67 sec(s) Loss: 43.223328\n",
      "Train: 8.927615, Valid: 8.892263\n",
      "[007/100] 9.77 sec(s) Loss: 43.059020\n",
      "Train: 8.906357, Valid: 8.869316\n",
      "[008/100] 9.69 sec(s) Loss: 42.930788\n",
      "[009/100] 9.71 sec(s) Loss: 42.800536\n",
      "Train: 8.888933, Valid: 8.866988\n",
      "[010/100] 9.73 sec(s) Loss: 42.702923\n",
      "Train: 8.871411, Valid: 8.858609\n",
      "[011/100] 9.71 sec(s) Loss: 42.575946\n",
      "[012/100] 10.70 sec(s) Loss: 42.557792\n",
      "Train: 8.858342, Valid: 8.840666\n",
      "[013/100] 10.09 sec(s) Loss: 42.499213\n",
      "Train: 8.848343, Valid: 8.832322\n",
      "[014/100] 9.76 sec(s) Loss: 42.461949\n",
      "[015/100] 9.69 sec(s) Loss: 42.379093\n",
      "[016/100] 9.77 sec(s) Loss: 42.307731\n",
      "Train: 8.829597, Valid: 8.831594\n",
      "[017/100] 10.65 sec(s) Loss: 42.244668\n",
      "[018/100] 9.77 sec(s) Loss: 42.242833\n",
      "[019/100] 9.79 sec(s) Loss: 42.220994\n",
      "[020/100] 9.74 sec(s) Loss: 42.130162\n",
      "Train: 8.809732, Valid: 8.831386\n",
      "[021/100] 9.75 sec(s) Loss: 42.132084\n",
      "[022/100] 9.79 sec(s) Loss: 42.080739\n",
      "[023/100] 10.69 sec(s) Loss: 41.978360\n",
      "[024/100] 369.01 sec(s) Loss: 42.018906\n",
      "Train: 8.800190, Valid: 8.826004\n",
      "[025/100] 9.78 sec(s) Loss: 41.955286\n",
      "[026/100] 9.67 sec(s) Loss: 41.957657\n",
      "[027/100] 9.70 sec(s) Loss: 41.910271\n",
      "Train: 8.800980, Valid: 8.819910\n",
      "[028/100] 10.61 sec(s) Loss: 41.923137\n",
      "[029/100] 9.64 sec(s) Loss: 41.894417\n",
      "Train: 8.788824, Valid: 8.815016\n",
      "[030/100] 9.75 sec(s) Loss: 41.807060\n",
      "[031/100] 9.64 sec(s) Loss: 41.796286\n",
      "[032/100] 9.72 sec(s) Loss: 41.821874\n",
      "[033/100] 9.69 sec(s) Loss: 41.875383\n",
      "[034/100] 10.65 sec(s) Loss: 41.797656\n",
      "[035/100] 9.70 sec(s) Loss: 41.766203\n",
      "[036/100] 9.78 sec(s) Loss: 41.729251\n",
      "[037/100] 9.70 sec(s) Loss: 41.804434\n",
      "[038/100] 9.77 sec(s) Loss: 41.767999\n",
      "Train: 8.769464, Valid: 8.812498\n",
      "Train: 8.774364, Valid: 8.811659\n",
      "[039/100] 10.68 sec(s) Loss: 41.765644\n",
      "[040/100] 9.73 sec(s) Loss: 41.676833\n",
      "[041/100] 9.80 sec(s) Loss: 41.598523\n",
      "Train: 8.766097, Valid: 8.794564\n",
      "[042/100] 9.68 sec(s) Loss: 41.758102\n",
      "[043/100] 17.65 sec(s) Loss: 41.608648\n",
      "[044/100] 9.76 sec(s) Loss: 41.702889\n",
      "[045/100] 10.71 sec(s) Loss: 41.605726\n",
      "[046/100] 9.67 sec(s) Loss: 41.610276\n",
      "[047/100] 9.71 sec(s) Loss: 41.590133\n",
      "[048/100] 9.75 sec(s) Loss: 41.545410\n",
      "[049/100] 9.70 sec(s) Loss: 41.559162\n",
      "[050/100] 10.66 sec(s) Loss: 41.551031\n",
      "[051/100] 9.98 sec(s) Loss: 41.569781\n",
      "[052/100] 9.72 sec(s) Loss: 41.581021\n",
      "[053/100] 9.70 sec(s) Loss: 41.495522\n",
      "Train: 9.739865, Valid: 9.694585\n",
      "Train: 9.421581, Valid: 9.362503\n",
      "Train: 9.302878, Valid: 9.264738\n",
      "Train: 9.220080, Valid: 9.159030\n",
      "[001/100] 9.79 sec(s) Loss: 84.451135\n",
      "Train: 9.168600, Valid: 9.104927\n",
      "Train: 9.126338, Valid: 9.068161\n",
      "Train: 9.086707, Valid: 9.043220\n",
      "Train: 9.066208, Valid: 9.019823\n",
      "[002/100] 9.81 sec(s) Loss: 75.736934\n",
      "Train: 9.045595, Valid: 8.987536\n",
      "Train: 9.029848, Valid: 8.980137\n",
      "Train: 9.017669, Valid: 8.959538\n",
      "[003/100] 9.78 sec(s) Loss: 74.187894\n",
      "Train: 8.989596, Valid: 8.954411\n",
      "Train: 8.970642, Valid: 8.949532\n",
      "[004/100] 10.25 sec(s) Loss: 73.537619\n",
      "Train: 8.962262, Valid: 8.911344\n",
      "Train: 8.954505, Valid: 8.908151\n",
      "Train: 8.945833, Valid: 8.907652\n",
      "[005/100] 10.17 sec(s) Loss: 73.090127\n",
      "Train: 8.929955, Valid: 8.902946\n",
      "Train: 8.918658, Valid: 8.893213\n",
      "[006/100] 10.98 sec(s) Loss: 72.705646\n",
      "[007/100] 10.34 sec(s) Loss: 72.450321\n",
      "Train: 8.901381, Valid: 8.884623\n",
      "[008/100] 11.14 sec(s) Loss: 72.341987\n",
      "Train: 8.886791, Valid: 8.877101\n",
      "Train: 8.896786, Valid: 8.874605\n",
      "[009/100] 10.58 sec(s) Loss: 71.962052\n",
      "Train: 8.877024, Valid: 8.856103\n",
      "[010/100] 9.94 sec(s) Loss: 71.891093\n",
      "[011/100] 9.79 sec(s) Loss: 71.544382\n",
      "[012/100] 11.15 sec(s) Loss: 71.584860\n",
      "Train: 8.855131, Valid: 8.852707\n",
      "Train: 8.850837, Valid: 8.848708\n",
      "[013/100] 10.11 sec(s) Loss: 71.520349\n",
      "Train: 8.850109, Valid: 8.843541\n",
      "[014/100] 10.03 sec(s) Loss: 71.297526\n",
      "[015/100] 10.01 sec(s) Loss: 71.230714\n",
      "[016/100] 10.02 sec(s) Loss: 71.217548\n",
      "Train: 8.851151, Valid: 8.839025\n",
      "Train: 8.825856, Valid: 8.826157\n",
      "[017/100] 11.40 sec(s) Loss: 71.006546\n",
      "[018/100] 10.56 sec(s) Loss: 71.145400\n",
      "[019/100] 10.62 sec(s) Loss: 70.949745\n",
      "[020/100] 10.61 sec(s) Loss: 70.923753\n",
      "Train: 8.824345, Valid: 8.821185\n",
      "[021/100] 10.30 sec(s) Loss: 71.002637\n",
      "[022/100] 10.35 sec(s) Loss: 70.817349\n",
      "[023/100] 11.21 sec(s) Loss: 70.674141\n",
      "[024/100] 9.99 sec(s) Loss: 70.741974\n",
      "[025/100] 9.93 sec(s) Loss: 70.714394\n",
      "[026/100] 9.82 sec(s) Loss: 70.519541\n",
      "[027/100] 9.73 sec(s) Loss: 70.546991\n",
      "[028/100] 10.72 sec(s) Loss: 70.511717\n",
      "[029/100] 9.71 sec(s) Loss: 70.482140\n",
      "[030/100] 9.73 sec(s) Loss: 70.448701\n",
      "[031/100] 9.77 sec(s) Loss: 70.356627\n",
      "Train: 8.790468, Valid: 8.821114\n",
      "[032/100] 9.74 sec(s) Loss: 70.341559\n",
      "[033/100] 9.77 sec(s) Loss: 70.376951\n",
      "[034/100] 10.67 sec(s) Loss: 70.212040\n",
      "[035/100] 9.76 sec(s) Loss: 70.375901\n",
      "[036/100] 9.74 sec(s) Loss: 70.350378\n",
      "[037/100] 9.74 sec(s) Loss: 70.265487\n",
      "[038/100] 9.98 sec(s) Loss: 70.168332\n",
      "[039/100] 12.18 sec(s) Loss: 70.206760\n",
      "[040/100] 10.59 sec(s) Loss: 70.157001\n",
      "[041/100] 10.03 sec(s) Loss: 70.151986\n",
      "[042/100] 266.10 sec(s) Loss: 70.081426\n",
      "[043/100] 11.66 sec(s) Loss: 69.984020\n",
      "Train: 9.749782, Valid: 9.725658\n",
      "Train: 9.430532, Valid: 9.383267\n",
      "Train: 9.320411, Valid: 9.265157\n",
      "Train: 9.243769, Valid: 9.175725\n",
      "[001/100] 9.85 sec(s) Loss: 93.135512\n",
      "Train: 9.185977, Valid: 9.129964\n",
      "Train: 9.152856, Valid: 9.077272\n",
      "Train: 9.120528, Valid: 9.072797\n",
      "Train: 9.094669, Valid: 9.045121\n",
      "[002/100] 9.73 sec(s) Loss: 83.885569\n",
      "Train: 9.076290, Valid: 9.018387\n",
      "Train: 9.049386, Valid: 9.001852\n",
      "Train: 9.042332, Valid: 9.001602\n",
      "Train: 9.033367, Valid: 8.988587\n",
      "[003/100] 9.76 sec(s) Loss: 82.128458\n",
      "Train: 9.004915, Valid: 8.962231\n",
      "Train: 9.001664, Valid: 8.953380\n",
      "Train: 8.995925, Valid: 8.943069\n",
      "[004/100] 9.86 sec(s) Loss: 81.353240\n",
      "Train: 8.975485, Valid: 8.923738\n",
      "Train: 8.963169, Valid: 8.914313\n",
      "[005/100] 9.83 sec(s) Loss: 80.873223\n",
      "Train: 8.956535, Valid: 8.907391\n",
      "Train: 8.948974, Valid: 8.904604\n",
      "Train: 8.943823, Valid: 8.900694\n",
      "[006/100] 10.87 sec(s) Loss: 80.326965\n",
      "Train: 8.937707, Valid: 8.894975\n",
      "Train: 8.928825, Valid: 8.893559\n",
      "Train: 8.933583, Valid: 8.883550\n",
      "Train: 8.922400, Valid: 8.882533\n",
      "[007/100] 9.84 sec(s) Loss: 80.043516\n",
      "Train: 8.917382, Valid: 8.880601\n",
      "[008/100] 9.88 sec(s) Loss: 79.864006\n",
      "[009/100] 9.77 sec(s) Loss: 79.547231\n",
      "Train: 8.894011, Valid: 8.875849\n",
      "Train: 8.886476, Valid: 8.871324\n",
      "[010/100] 9.92 sec(s) Loss: 79.380849\n",
      "Train: 8.882659, Valid: 8.860209\n",
      "Train: 8.880112, Valid: 8.855116\n",
      "[011/100] 9.86 sec(s) Loss: 79.124388\n",
      "[012/100] 10.85 sec(s) Loss: 79.098896\n",
      "[013/100] 10.06 sec(s) Loss: 78.915900\n",
      "Train: 8.867791, Valid: 8.848898\n",
      "[014/100] 9.76 sec(s) Loss: 78.810250\n",
      "[015/100] 9.85 sec(s) Loss: 78.711726\n",
      "Train: 8.855024, Valid: 8.846374\n",
      "[016/100] 10.44 sec(s) Loss: 78.547930\n",
      "[017/100] 11.57 sec(s) Loss: 78.571059\n",
      "Train: 8.835768, Valid: 8.839000\n",
      "[018/100] 10.30 sec(s) Loss: 78.321934\n",
      "Train: 8.827930, Valid: 8.831741\n",
      "[019/100] 10.11 sec(s) Loss: 78.374815\n",
      "[020/100] 9.86 sec(s) Loss: 78.187370\n",
      "[021/100] 9.83 sec(s) Loss: 78.172072\n",
      "[022/100] 9.83 sec(s) Loss: 78.191576\n",
      "[023/100] 10.90 sec(s) Loss: 78.140983\n",
      "[024/100] 9.90 sec(s) Loss: 77.927443\n",
      "Train: 8.820130, Valid: 8.830361\n",
      "[025/100] 9.86 sec(s) Loss: 77.837103\n",
      "[026/100] 9.86 sec(s) Loss: 77.844302\n",
      "Train: 8.796580, Valid: 8.826743\n",
      "[027/100] 9.79 sec(s) Loss: 77.768028\n",
      "[028/100] 10.82 sec(s) Loss: 77.771351\n",
      "Train: 8.800620, Valid: 8.825867\n",
      "Train: 8.794746, Valid: 8.824264\n",
      "[029/100] 9.85 sec(s) Loss: 77.719783\n",
      "Train: 8.791667, Valid: 8.811936\n",
      "Train: 8.799835, Valid: 8.810086\n",
      "[030/100] 9.87 sec(s) Loss: 77.548824\n",
      "[031/100] 9.77 sec(s) Loss: 77.675063\n",
      "Train: 8.798120, Valid: 8.802029\n",
      "[032/100] 9.89 sec(s) Loss: 77.576254\n",
      "[033/100] 9.91 sec(s) Loss: 77.544284\n",
      "[034/100] 10.93 sec(s) Loss: 77.455960\n",
      "[035/100] 12.04 sec(s) Loss: 77.444071\n",
      "[036/100] 9.83 sec(s) Loss: 77.427764\n",
      "[037/100] 15.60 sec(s) Loss: 77.478437\n",
      "[038/100] 9.77 sec(s) Loss: 77.424299\n",
      "[039/100] 11.22 sec(s) Loss: 77.486226\n",
      "[040/100] 10.35 sec(s) Loss: 77.277084\n",
      "[041/100] 10.72 sec(s) Loss: 77.279234\n",
      "[042/100] 10.08 sec(s) Loss: 77.303221\n",
      "[043/100] 9.78 sec(s) Loss: 77.234469\n"
     ]
    }
   ],
   "source": [
    "z = [0.0, 0.1, 0.5, 0.9, 1.0]\n",
    "for i in z:\n",
    "    training_q7(i, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 0.0, test RMSE = 9.017365\n",
      "H = 0.1, test RMSE = 8.78764\n",
      "H = 0.5, test RMSE = 8.770102\n",
      "H = 0.9, test RMSE = 8.802077\n",
      "H = 1.0, test RMSE = 8.792296\n"
     ]
    }
   ],
   "source": [
    "for i in z:\n",
    "    model = torch.load('best_q7_z'+str(i)+'.pkl')\n",
    "    model.eval()\n",
    "    test_pred = model(torch.FloatTensor(X_test))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "    print('H = '+str(i)+', test RMSE = '+str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customerized(nn.Module):\n",
    "    def __init__(self, z):\n",
    "        super().__init__()\n",
    "        self.z = z\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        return self.z * torch.sum(torch.pow((x - y), 2)) + (1 - self.z) * torch.sum(0.5 * torch.clamp(x - y, min=0.0) + 0.5 * torch.clamp(y - x, min=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_q8(z, device):\n",
    "    model = MLP_q5(90).to(device)\n",
    "    loss = Customerized(z)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "    num_epoch = 100\n",
    "\n",
    "    flag = 0\n",
    "    batch_cnt = 0\n",
    "    best_step_cnt = 0\n",
    "    best_rmse = math.inf\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = torch.reshape(model(data[0]), (len(data[0]), ))\n",
    "            batch_loss = loss(train_pred, data[1])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += batch_loss.item()\n",
    "            \n",
    "\n",
    "            batch_cnt += 1\n",
    "            best_step_cnt += 1\n",
    "            if batch_cnt % 100 == 0:\n",
    "                train_pred = model(torch.FloatTensor(X_subtrain))\n",
    "                train_rmse = np.sqrt(mean_squared_error(train_pred.detach().numpy(), Y_subtrain))\n",
    "                valid_pred = model(torch.FloatTensor(X_valid))\n",
    "                valid_rmse = np.sqrt(mean_squared_error(valid_pred.detach().numpy(), Y_valid))\n",
    "                if valid_rmse < best_rmse:\n",
    "                    print('Train: %f, Valid: %f' % (train_rmse, valid_rmse))\n",
    "                    best_rmse = valid_rmse\n",
    "                    best_step_cnt = 0\n",
    "                    torch.save(model, 'best_q8_z'+str(z)+'.pkl')\n",
    "            if best_step_cnt >= 5000:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Loss: %3.6f' % \\\n",
    "          (epoch + 1, num_epoch, time.time()-epoch_start_time, train_loss/train_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9.734010, Valid: 9.675782\n",
      "Train: 9.448947, Valid: 9.391056\n",
      "Train: 9.316045, Valid: 9.273231\n",
      "Train: 9.244974, Valid: 9.204953\n",
      "[001/100] 10.06 sec(s) Loss: 12.349944\n",
      "Train: 9.186489, Valid: 9.135624\n",
      "Train: 9.143822, Valid: 9.106217\n",
      "Train: 9.123127, Valid: 9.072780\n",
      "Train: 9.101375, Valid: 9.059466\n",
      "[002/100] 9.86 sec(s) Loss: 11.249713\n",
      "Train: 9.066452, Valid: 9.008919\n",
      "Train: 9.038618, Valid: 9.004548\n",
      "Train: 9.028793, Valid: 9.000247\n",
      "[003/100] 9.86 sec(s) Loss: 11.030298\n",
      "Train: 9.010324, Valid: 8.969580\n",
      "Train: 8.998832, Valid: 8.946953\n",
      "[004/100] 9.74 sec(s) Loss: 10.917136\n",
      "Train: 8.978270, Valid: 8.940965\n",
      "Train: 8.977426, Valid: 8.938670\n",
      "Train: 8.975590, Valid: 8.933332\n",
      "[005/100] 9.84 sec(s) Loss: 10.871514\n",
      "Train: 8.949886, Valid: 8.920117\n",
      "Train: 8.937027, Valid: 8.904657\n",
      "[006/100] 10.76 sec(s) Loss: 10.805790\n",
      "[007/100] 9.83 sec(s) Loss: 10.780014\n",
      "Train: 8.912510, Valid: 8.882490\n",
      "[008/100] 9.76 sec(s) Loss: 10.734988\n",
      "Train: 8.896894, Valid: 8.862901\n",
      "[009/100] 9.73 sec(s) Loss: 10.716116\n",
      "[010/100] 9.79 sec(s) Loss: 10.696970\n",
      "[011/100] 9.78 sec(s) Loss: 10.668733\n",
      "Train: 8.862137, Valid: 8.837317\n",
      "[012/100] 10.78 sec(s) Loss: 10.652375\n",
      "[013/100] 9.77 sec(s) Loss: 10.637150\n",
      "[014/100] 9.90 sec(s) Loss: 10.614104\n",
      "[015/100] 9.80 sec(s) Loss: 10.609180\n",
      "[016/100] 9.82 sec(s) Loss: 10.608649\n",
      "[017/100] 10.70 sec(s) Loss: 10.593927\n",
      "[018/100] 9.78 sec(s) Loss: 10.567332\n",
      "[019/100] 9.86 sec(s) Loss: 10.557370\n",
      "Train: 8.828010, Valid: 8.830538\n",
      "[020/100] 9.92 sec(s) Loss: 10.544015\n",
      "[021/100] 9.85 sec(s) Loss: 10.548162\n",
      "[022/100] 9.78 sec(s) Loss: 10.547685\n",
      "[023/100] 10.77 sec(s) Loss: 10.536612\n",
      "[024/100] 9.79 sec(s) Loss: 10.515430\n",
      "[025/100] 9.85 sec(s) Loss: 10.519439\n",
      "Train: 8.814577, Valid: 8.830256\n",
      "[026/100] 9.81 sec(s) Loss: 10.514385\n",
      "[027/100] 9.84 sec(s) Loss: 10.502391\n",
      "Train: 8.797841, Valid: 8.815593\n",
      "[028/100] 10.80 sec(s) Loss: 10.498394\n",
      "Train: 8.800126, Valid: 8.815457\n",
      "[029/100] 9.80 sec(s) Loss: 10.497781\n",
      "[030/100] 9.83 sec(s) Loss: 10.499226\n",
      "[031/100] 9.78 sec(s) Loss: 10.489681\n",
      "[032/100] 9.84 sec(s) Loss: 10.495530\n",
      "Train: 8.789455, Valid: 8.812651\n",
      "[033/100] 9.79 sec(s) Loss: 10.489495\n",
      "[034/100] 10.79 sec(s) Loss: 10.483571\n",
      "[035/100] 9.80 sec(s) Loss: 10.478791\n",
      "[036/100] 9.79 sec(s) Loss: 10.475454\n",
      "[037/100] 9.82 sec(s) Loss: 10.458199\n",
      "[038/100] 9.82 sec(s) Loss: 10.461600\n",
      "[039/100] 10.75 sec(s) Loss: 10.449287\n",
      "[040/100] 9.75 sec(s) Loss: 10.458443\n",
      "[041/100] 9.81 sec(s) Loss: 10.456074\n",
      "[042/100] 9.80 sec(s) Loss: 10.453479\n",
      "[043/100] 9.86 sec(s) Loss: 10.440211\n",
      "Train: 8.772061, Valid: 8.803428\n",
      "[044/100] 9.79 sec(s) Loss: 10.442351\n",
      "[045/100] 10.71 sec(s) Loss: 10.427870\n",
      "[046/100] 9.80 sec(s) Loss: 10.433211\n",
      "[047/100] 9.73 sec(s) Loss: 10.423802\n",
      "[048/100] 9.83 sec(s) Loss: 10.438326\n",
      "[049/100] 9.75 sec(s) Loss: 10.407994\n",
      "[050/100] 10.74 sec(s) Loss: 10.415047\n",
      "[051/100] 9.85 sec(s) Loss: 10.429053\n",
      "[052/100] 9.82 sec(s) Loss: 10.432734\n",
      "[053/100] 9.75 sec(s) Loss: 10.420875\n",
      "[054/100] 9.76 sec(s) Loss: 10.401208\n",
      "[055/100] 9.80 sec(s) Loss: 10.410476\n",
      "Train: 9.716683, Valid: 9.689581\n",
      "Train: 9.413260, Valid: 9.364939\n",
      "Train: 9.303052, Valid: 9.267551\n",
      "Train: 9.233879, Valid: 9.174485\n",
      "[001/100] 9.88 sec(s) Loss: 48.184531\n",
      "Train: 9.178095, Valid: 9.122103\n",
      "Train: 9.137657, Valid: 9.104693\n",
      "Train: 9.099289, Valid: 9.065784\n",
      "Train: 9.076386, Valid: 9.035047\n",
      "[002/100] 9.84 sec(s) Loss: 43.438915\n",
      "Train: 9.061914, Valid: 9.009188\n",
      "Train: 9.042823, Valid: 9.001124\n",
      "Train: 9.028988, Valid: 8.969392\n",
      "Train: 9.017589, Valid: 8.967147\n",
      "[003/100] 9.82 sec(s) Loss: 42.575281\n",
      "Train: 8.995875, Valid: 8.954505\n",
      "[004/100] 9.79 sec(s) Loss: 42.158967\n",
      "Train: 8.976098, Valid: 8.929629\n",
      "Train: 8.964376, Valid: 8.916977\n",
      "[005/100] 9.88 sec(s) Loss: 41.919327\n",
      "Train: 8.940320, Valid: 8.916317\n",
      "Train: 8.931581, Valid: 8.899225\n",
      "[006/100] 10.97 sec(s) Loss: 41.709847\n",
      "Train: 8.924558, Valid: 8.890092\n",
      "[007/100] 10.26 sec(s) Loss: 41.522447\n",
      "Train: 8.902102, Valid: 8.882360\n",
      "[008/100] 9.81 sec(s) Loss: 41.364190\n",
      "Train: 8.897899, Valid: 8.865928\n",
      "[009/100] 9.88 sec(s) Loss: 41.284818\n",
      "Train: 8.891396, Valid: 8.862807\n",
      "[010/100] 9.81 sec(s) Loss: 41.212900\n",
      "Train: 8.884403, Valid: 8.853388\n",
      "[011/100] 9.88 sec(s) Loss: 41.162858\n",
      "[012/100] 10.72 sec(s) Loss: 41.037321\n",
      "Train: 8.861535, Valid: 8.843771\n",
      "[013/100] 9.80 sec(s) Loss: 40.915736\n",
      "[014/100] 9.84 sec(s) Loss: 40.834555\n",
      "[015/100] 9.81 sec(s) Loss: 40.890551\n",
      "Train: 8.844598, Valid: 8.833030\n",
      "Train: 8.844207, Valid: 8.829060\n",
      "[016/100] 9.85 sec(s) Loss: 40.749230\n",
      "[017/100] 10.74 sec(s) Loss: 40.718161\n",
      "[018/100] 9.84 sec(s) Loss: 40.713378\n",
      "[019/100] 9.84 sec(s) Loss: 40.673725\n",
      "[020/100] 9.83 sec(s) Loss: 40.593547\n",
      "[021/100] 9.80 sec(s) Loss: 40.565760\n",
      "[022/100] 9.79 sec(s) Loss: 40.588738\n",
      "Train: 8.814284, Valid: 8.824886\n",
      "[023/100] 10.83 sec(s) Loss: 40.453742\n",
      "[024/100] 9.80 sec(s) Loss: 40.489455\n",
      "Train: 8.800751, Valid: 8.823905\n",
      "[025/100] 9.85 sec(s) Loss: 40.485227\n",
      "Train: 8.799116, Valid: 8.807600\n",
      "[026/100] 9.76 sec(s) Loss: 40.457242\n",
      "[027/100] 9.85 sec(s) Loss: 40.395704\n",
      "[028/100] 10.74 sec(s) Loss: 40.410723\n",
      "[029/100] 9.88 sec(s) Loss: 40.411240\n",
      "[030/100] 9.83 sec(s) Loss: 40.304493\n",
      "[031/100] 9.77 sec(s) Loss: 40.345336\n",
      "[032/100] 9.83 sec(s) Loss: 40.287246\n",
      "[033/100] 9.83 sec(s) Loss: 40.230653\n",
      "[034/100] 401.64 sec(s) Loss: 40.341767\n",
      "Train: 8.779021, Valid: 8.804597\n",
      "[035/100] 10.18 sec(s) Loss: 40.263425\n",
      "[036/100] 9.97 sec(s) Loss: 40.211010\n",
      "[037/100] 9.82 sec(s) Loss: 40.218526\n",
      "[038/100] 9.83 sec(s) Loss: 40.175761\n",
      "[039/100] 10.74 sec(s) Loss: 40.177869\n",
      "[040/100] 9.78 sec(s) Loss: 40.226136\n",
      "[041/100] 9.81 sec(s) Loss: 40.121240\n",
      "[042/100] 9.74 sec(s) Loss: 40.104552\n",
      "[043/100] 9.84 sec(s) Loss: 40.163572\n",
      "[044/100] 9.78 sec(s) Loss: 40.072890\n",
      "[045/100] 10.74 sec(s) Loss: 40.092729\n",
      "[046/100] 9.79 sec(s) Loss: 40.067992\n",
      "Train: 9.718952, Valid: 9.687487\n",
      "Train: 9.420266, Valid: 9.388206\n",
      "Train: 9.306467, Valid: 9.237068\n",
      "Train: 9.231728, Valid: 9.181466\n",
      "[001/100] 9.89 sec(s) Loss: 83.886589\n",
      "Train: 9.180528, Valid: 9.108574\n",
      "Train: 9.147878, Valid: 9.093286\n",
      "Train: 9.100828, Valid: 9.058540\n",
      "Train: 9.082024, Valid: 9.026503\n",
      "[002/100] 9.85 sec(s) Loss: 75.564227\n",
      "Train: 9.062195, Valid: 8.992599\n",
      "Train: 9.012527, Valid: 8.982804\n",
      "[003/100] 9.84 sec(s) Loss: 74.146272\n",
      "Train: 9.002927, Valid: 8.962449\n",
      "Train: 9.000586, Valid: 8.948422\n",
      "[004/100] 9.84 sec(s) Loss: 73.345802\n",
      "Train: 8.971910, Valid: 8.933474\n",
      "Train: 8.962737, Valid: 8.932714\n",
      "[005/100] 9.91 sec(s) Loss: 72.873517\n",
      "Train: 8.952990, Valid: 8.920286\n",
      "Train: 8.951507, Valid: 8.914708\n",
      "Train: 8.933351, Valid: 8.880652\n",
      "[006/100] 10.76 sec(s) Loss: 72.451292\n",
      "Train: 8.919849, Valid: 8.877163\n",
      "[007/100] 9.84 sec(s) Loss: 72.257849\n",
      "Train: 8.907770, Valid: 8.876838\n",
      "[008/100] 9.78 sec(s) Loss: 71.804783\n",
      "[009/100] 9.85 sec(s) Loss: 71.822801\n",
      "Train: 8.875052, Valid: 8.866954\n",
      "[010/100] 9.78 sec(s) Loss: 71.642200\n",
      "Train: 8.882823, Valid: 8.853461\n",
      "[011/100] 9.78 sec(s) Loss: 71.478418\n",
      "[012/100] 10.78 sec(s) Loss: 71.354060\n",
      "[013/100] 9.83 sec(s) Loss: 71.415013\n",
      "[014/100] 9.82 sec(s) Loss: 71.105844\n",
      "[015/100] 9.79 sec(s) Loss: 71.045083\n",
      "Train: 8.846182, Valid: 8.849862\n",
      "[016/100] 9.87 sec(s) Loss: 70.982991\n",
      "Train: 8.840751, Valid: 8.835648\n",
      "Train: 8.840316, Valid: 8.828531\n",
      "[017/100] 10.73 sec(s) Loss: 70.889309\n",
      "[018/100] 9.81 sec(s) Loss: 70.873394\n",
      "[019/100] 9.84 sec(s) Loss: 70.696290\n",
      "[020/100] 10.07 sec(s) Loss: 70.710485\n",
      "[021/100] 9.86 sec(s) Loss: 70.605911\n",
      "[022/100] 9.81 sec(s) Loss: 70.532197\n",
      "[023/100] 10.83 sec(s) Loss: 70.546181\n",
      "[024/100] 9.81 sec(s) Loss: 70.548965\n",
      "Train: 8.800701, Valid: 8.808308\n",
      "[025/100] 9.90 sec(s) Loss: 70.423836\n",
      "[026/100] 9.75 sec(s) Loss: 70.367432\n",
      "[027/100] 9.79 sec(s) Loss: 70.286979\n",
      "[028/100] 10.78 sec(s) Loss: 70.137237\n",
      "[029/100] 9.79 sec(s) Loss: 70.369792\n",
      "[030/100] 606.60 sec(s) Loss: 70.280145\n",
      "[031/100] 10.04 sec(s) Loss: 70.137300\n",
      "[032/100] 9.93 sec(s) Loss: 70.102028\n",
      "[033/100] 9.79 sec(s) Loss: 70.083604\n",
      "[034/100] 10.67 sec(s) Loss: 70.127437\n",
      "[035/100] 9.76 sec(s) Loss: 70.132030\n",
      "[036/100] 9.77 sec(s) Loss: 70.029983\n",
      "Train: 9.745414, Valid: 9.705647\n",
      "Train: 9.428857, Valid: 9.342879\n",
      "Train: 9.318914, Valid: 9.262504\n",
      "Train: 9.247601, Valid: 9.179779\n",
      "[001/100] 9.79 sec(s) Loss: 92.966418\n",
      "Train: 9.185287, Valid: 9.141762\n",
      "Train: 9.150433, Valid: 9.088065\n",
      "Train: 9.111142, Valid: 9.053949\n",
      "Train: 9.085917, Valid: 9.041114\n",
      "[002/100] 9.74 sec(s) Loss: 83.851957\n",
      "Train: 9.065569, Valid: 9.017482\n",
      "Train: 9.049902, Valid: 9.000818\n",
      "Train: 9.015822, Valid: 8.983721\n",
      "[003/100] 9.76 sec(s) Loss: 82.063738\n",
      "Train: 8.999506, Valid: 8.954001\n",
      "[004/100] 9.89 sec(s) Loss: 81.099740\n",
      "Train: 8.975162, Valid: 8.939532\n",
      "Train: 8.967887, Valid: 8.919956\n",
      "[005/100] 9.71 sec(s) Loss: 80.679394\n",
      "Train: 8.940340, Valid: 8.917801\n",
      "Train: 8.943339, Valid: 8.905770\n",
      "Train: 8.935023, Valid: 8.892754\n",
      "[006/100] 258.77 sec(s) Loss: 80.054849\n",
      "Train: 8.920651, Valid: 8.890084\n",
      "[007/100] 10.33 sec(s) Loss: 79.920275\n",
      "Train: 8.907384, Valid: 8.875459\n",
      "Train: 8.897078, Valid: 8.872731\n",
      "[008/100] 10.01 sec(s) Loss: 79.524945\n",
      "Train: 8.903014, Valid: 8.866407\n",
      "[009/100] 9.87 sec(s) Loss: 79.295555\n",
      "[010/100] 9.81 sec(s) Loss: 79.067933\n",
      "Train: 8.874804, Valid: 8.845850\n",
      "[011/100] 9.77 sec(s) Loss: 78.937319\n",
      "[012/100] 10.76 sec(s) Loss: 78.740223\n",
      "[013/100] 9.80 sec(s) Loss: 78.700898\n",
      "Train: 8.844414, Valid: 8.845293\n",
      "[014/100] 9.77 sec(s) Loss: 78.649330\n",
      "Train: 8.828500, Valid: 8.832816\n",
      "[015/100] 9.84 sec(s) Loss: 78.378081\n",
      "Train: 8.831509, Valid: 8.831580\n",
      "[016/100] 9.80 sec(s) Loss: 78.357819\n",
      "[017/100] 10.77 sec(s) Loss: 78.260979\n",
      "[018/100] 9.76 sec(s) Loss: 78.289612\n",
      "Train: 8.821590, Valid: 8.823818\n",
      "[019/100] 9.81 sec(s) Loss: 78.187003\n",
      "Train: 8.816668, Valid: 8.812029\n",
      "[020/100] 9.77 sec(s) Loss: 78.011922\n",
      "[021/100] 9.83 sec(s) Loss: 77.938214\n",
      "[022/100] 9.88 sec(s) Loss: 77.883705\n",
      "[023/100] 10.67 sec(s) Loss: 77.901232\n",
      "[024/100] 9.77 sec(s) Loss: 77.699814\n",
      "[025/100] 9.81 sec(s) Loss: 77.648493\n",
      "[026/100] 7208.67 sec(s) Loss: 77.569462\n",
      "[027/100] 5327.27 sec(s) Loss: 77.583089\n",
      "[028/100] 12.03 sec(s) Loss: 77.656550\n",
      "[029/100] 10.18 sec(s) Loss: 77.545006\n",
      "[030/100] 10.44 sec(s) Loss: 77.704763\n",
      "[031/100] 10.73 sec(s) Loss: 77.348572\n"
     ]
    }
   ],
   "source": [
    "z = [0.1, 0.5, 0.9, 1.0]\n",
    "for i in z:\n",
    "    training_q8(i, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 0.1, test RMSE = 8.760516\n",
      "z = 0.5, test RMSE = 8.786251\n",
      "z = 0.9, test RMSE = 8.77406\n",
      "z = 1.0, test RMSE = 8.794384\n"
     ]
    }
   ],
   "source": [
    "for i in z:\n",
    "    model = torch.load('best_q8_z'+str(i)+'.pkl')\n",
    "    model.eval()\n",
    "    test_pred = model(torch.FloatTensor(X_test))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_pred.detach().numpy(), Y_test))\n",
    "    print('z = '+str(i)+', test RMSE = '+str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
